# -*- coding: utf-8 -*-
"""Bayesian_Neural_Network_for_Probabilistic_VTEC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18_twiOz0naNhq-XLzCTdr0sLnrgRCF_M

In this notebook, two Bayesian neural network models are developed for VTEC forecasting with 95% confidence intervals, from the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M., submitted to the Space Weather Jornal, AGU. In both models, the deterministic network parameters (weights) are replaced by probability distributions of these weights. However, the first model only describes the uncertainty in the weights and models the model uncertainty, while the second model also estimate the data uncertainty by providing probabilistic output estimate via the negative log-likelihood (NLL) loss.

The notebook was created by Randa Natras: randa.natras@hotmail.com; randa.natras@tum.de

## Installation

The deep learning frameworks Tensorflow/Keras are considered with [TensorFlow Probability](https://www.tensorflow.org/probability) library.  The implementation is done in Google Colab. If you're outside of Colab, you may install it with pip:

```python
pip install tensorflow-probability
```

### **Libraries**
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_datasets as tfds
import tensorflow_probability as tfp

import numpy as np
import pandas as pd
import datetime
import matplotlib.dates as mdates
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats
from scipy import stats 
from sklearn import metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

!rm -rvf logs

logdir = "logs"

from tensorboard.plugins.hparams import api as hp

"""## The dataset

The dataset from the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M.
"""

data_df = pd.read_csv ('Training_data_VTEC.csv', delimiter=';')
data_test_df= pd.read_csv ('Test_data_VTEC.csv', delimiter=';')

data_df.head()

data_test_df.head()

data_df['Date-time'] = (pd.to_datetime(data_df['Year'] * 1000 + data_df['DOY'], format='%Y%j') + pd.to_timedelta(data_df['Hour '], unit='h'))
data_test_df['Date-time'] = (pd.to_datetime(data_test_df['Year'] * 1000 + data_test_df['DOY'], format='%Y%j') + pd.to_timedelta(data_test_df['Hour '], unit='h'))

data_df_2 = data_df.set_index('Date-time')
data_test_df_2 = data_test_df.set_index('Date-time')

data_df_2.head()

data_test_df_2.head()

"""### Create training and evaluation datasets"""

#Calculation of the exponential moving average over the last 30 days and 4 days
def compute_exponential_moving_average (data_frame, input_col, new_col_EMA_30d, new_col_EMA_96h):
  ema_30d = data_frame[input_col].ewm(span=(30*24), adjust=False).mean()
  ema_96h = data_frame[input_col].ewm(span=(4*24), adjust=False).mean()
  data_frame[new_col_EMA_30d] = ema_30d
  data_frame[new_col_EMA_96h] = ema_96h

#Calculation of the first and second VTEC derivatives
def compute_derivatives (data_frame, input_col, new_col_der1, new_col_der2):
  der1 = data_frame[input_col].diff()
  der2 = data_frame[input_col].diff().diff()
  data_frame[new_col_der1] = der1
  data_frame[new_col_der2] = der2

compute_exponential_moving_average (data_df_2, 'VTEC_10N', 'VTEC_10N_EMA(30d)', 'VTEC_10N_EMA(96h)')
compute_exponential_moving_average (data_df_2, 'VTEC_40N', 'VTEC_40N_EMA(30d)', 'VTEC_40N_EMA(96h)')
compute_exponential_moving_average (data_df_2, 'VTEC_70N', 'VTEC_70N_EMA(30d)', 'VTEC_70N_EMA(96h)')

compute_exponential_moving_average (data_test_df_2, 'VTEC_10N', 'VTEC_10N_EMA(30d)', 'VTEC_10N_EMA(96h)')
compute_exponential_moving_average (data_test_df_2, 'VTEC_40N', 'VTEC_40N_EMA(30d)', 'VTEC_40N_EMA(96h)')
compute_exponential_moving_average (data_test_df_2, 'VTEC_70N', 'VTEC_70N_EMA(30d)', 'VTEC_70N_EMA(96h)')

compute_derivatives (data_df_2, 'VTEC_10N', 'VTEC_10N_der1', 'VTEC_10N_der2')
compute_derivatives (data_df_2, 'VTEC_40N', 'VTEC_40N_der1', 'VTEC_40N_der2')
compute_derivatives (data_df_2, 'VTEC_70N', 'VTEC_70N_der1', 'VTEC_70N_der2')

compute_derivatives (data_test_df_2, 'VTEC_10N', 'VTEC_10N_der1', 'VTEC_10N_der2')
compute_derivatives (data_test_df_2, 'VTEC_40N', 'VTEC_40N_der1', 'VTEC_40N_der2')
compute_derivatives (data_test_df_2, 'VTEC_70N', 'VTEC_70N_der1', 'VTEC_70N_der2')

#Calculation of the new features leads to NaN values, which are now omitted
data_df_2.dropna(inplace = True)
data_test_df_2.dropna(inplace = True)

"""Next, cyclical continuous features Hour and DOY are transformed into 2 dimensions with sine and cosine.

"""

def encode(data, col, max_val):
    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)
    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)
    return data

#Converting the cyclical continuous features DOY and Hour into sine and cosine

def encode_cyclical (data_frame, input_col_hour, input_col_doy):

  value_hour = 24
  value_doy = 365.25

  data_frame [input_col_hour + ' (sin)'] = np.sin (2 * np.pi * data_frame[input_col_hour] / value_hour)
  data_frame [input_col_hour + ' (cos)'] = np.cos (2 * np.pi * data_frame[input_col_hour] / value_hour)
  data_frame [input_col_doy + ' (sin)'] = np.sin (2 * np.pi * data_frame[input_col_doy] / value_doy)
  data_frame [input_col_doy + ' (cos)'] = np.cos (2 * np.pi * data_frame[input_col_doy] / value_doy)
  
  data_frame = data_frame.drop([input_col_hour, input_col_doy], axis=1)

encode_cyclical (data_df_2, 'Hour ', 'DOY')
encode_cyclical (data_test_df_2, 'Hour ', 'DOY')

data_df_2.head()

data_test_df_2.head()

"""Preparation of the data for training and testing the model: extraction of the X and Y variables."""

X_10_df=data_df_2.drop (['Year','VTEC_40N', 'VTEC_70N','VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)',  'VTEC_40N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_70N_EMA(96h)', 'VTEC_40N_der1', 'VTEC_70N_der1', 'VTEC_40N_der2', 'VTEC_70N_der2'], axis=1)
X_40_df=data_df_2.drop (['Year','VTEC_10N', 'VTEC_70N','VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)', 'VTEC_10N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_10N_EMA(96h)', 'VTEC_70N_EMA(96h)', 'VTEC_10N_der1', 'VTEC_70N_der1', 'VTEC_10N_der2', 'VTEC_70N_der2' ], axis=1)
X_70_df=data_df_2.drop (['Year','VTEC_10N', 'VTEC_40N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)' ,  'VTEC_40N_EMA(30d)', 'VTEC_10N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_10N_EMA(96h)',  'VTEC_40N_der1', 'VTEC_10N_der1', 'VTEC_40N_der2', 'VTEC_10N_der2'], axis=1)
X_10_test_df=data_test_df_2.drop (['Year','VTEC_40N', 'VTEC_70N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)', 'VTEC_40N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_70N_EMA(96h)',  'VTEC_40N_der1', 'VTEC_70N_der1', 'VTEC_40N_der2', 'VTEC_70N_der2' ], axis=1)
X_40_test_df=data_test_df_2.drop (['Year','VTEC_10N', 'VTEC_70N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)' , 'VTEC_10N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_10N_EMA(96h)', 'VTEC_70N_EMA(96h)', 'VTEC_10N_der1', 'VTEC_70N_der1', 'VTEC_10N_der2', 'VTEC_70N_der2'], axis=1)
X_70_test_df=data_test_df_2.drop (['Year','VTEC_10N', 'VTEC_40N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)' , 'VTEC_40N_EMA(30d)', 'VTEC_10N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_10N_EMA(96h)', 'VTEC_40N_der1', 'VTEC_10N_der1', 'VTEC_40N_der2', 'VTEC_10N_der2'], axis=1)

X_10_df.head()

X_10_test_df.head()

X_10 = X_10_df.to_numpy()
X_40 = X_40_df.to_numpy()
X_70 = X_70_df.to_numpy()
X_10_test_new = X_10_test_df.to_numpy()
X_40_test_new = X_40_test_df.to_numpy()
X_70_test_new = X_70_test_df.to_numpy()

train_y_10 = data_df_2 ['VTEC_10N (t+24)'].to_numpy()
train_y_40 = data_df_2 ['VTEC_40N (t+24)'].to_numpy()
train_y_70 = data_df_2 ['VTEC_70N (t+24)'].to_numpy()
y_10_test_new = data_test_df_2 ['VTEC_10N (t+24)'].to_numpy()
y_40_test_new = data_test_df_2 ['VTEC_40N (t+24)'].to_numpy()
y_70_test_new = data_test_df_2 ['VTEC_70N (t+24)'].to_numpy()

#Data standardization
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler_70 = scaler.fit(X_70)
scaler_40= scaler.fit(X_40)
scaler_10 = scaler.fit(X_10)

train_X_70_scaled = scaler_70.transform(X_70)
train_X_40_scaled = scaler_40.transform(X_40)
train_X_10_scaled = scaler_10.transform(X_10)

test_X_70_scaled = scaler_70.transform(X_70_test_new)
test_X_40_scaled = scaler_40.transform(X_40_test_new)
test_X_10_scaled = scaler_10.transform(X_10_test_new)

"""## Standard artificial neural network

A standard deterministic artificial neural network model (MLP) is created as a baseline with the ANN architecture from the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M.: 1 hidden layer with 32 neurons.
"""

def create_1layer_model(input_dim, hidden_dim):
    inputs = layers.Input(shape=(input_dim,))

    # Hidden layer with deterministic weights using a Dense layer and non linear activation function
    features = layers.Dense(hidden_dim, activation="sigmoid")(inputs)

    # The output is deterministic: a single point estimate.
    outputs = layers.Dense(units=1)(features)

    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=20)

learning_rate = 0.001

def run_experiment(model, loss, num_epochs, num_batch_size, X_train, y_train):
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )
    
    history=model.fit(X_train, y_train, epochs=num_epochs, batch_size=num_batch)
    _, rmset = model.evaluate(X_train, y_train, verbose=0)
    print(f"Train RMSE: {round(rmset, 3)}")

MLP_model_10 = create_1layer_model(16,32)
MLP_model_10.summary()

MLP_model_40 = create_1layer_model(16,32)
MLP_model_70 = create_1layer_model(16,32)

#Model training and cross-validation for VTEC point located at 10E 10N
num_epochs = 1000
num_batch= 500
mse_loss = keras.losses.MeanSquaredError()
run_experiment(MLP_model_10, mse_loss, num_epochs, num_batch, train_X_10_scaled, train_y_10)

#Model training and cross-validation for VTEC point located at 10E 40N
num_epochs = 1000
num_batch= 500
mse_loss = keras.losses.MeanSquaredError()
run_experiment(MLP_model_40, mse_loss, num_epochs, num_batch, train_X_40_scaled, train_y_40)

#Model training and cross-validation for VTEC point located at 10E 70N
num_epochs = 1000
num_batch= 500
mse_loss = keras.losses.MeanSquaredError()
run_experiment(MLP_model_70, mse_loss, num_epochs, num_batch, train_X_70_scaled, train_y_70)

"""### **Testing**

We take the test set to obtain predictions for the previously unseen data.
Since the ANN model is deterministic, we get a single a
point estimate for each test example, with no information about the
uncertainty of the model nor the prediction.
"""

import time
start = time.time()

y_pred_10 = MLP_model_10.predict(test_X_10_scaled).flatten()
y_pred_40 = MLP_model_40.predict(test_X_40_scaled).flatten()
y_pred_70 = MLP_model_70.predict(test_X_70_scaled).flatten()

end = time.time()
print(end - start, "seconds")

from tensorflow.keras.models import save_model
save_model(MLP_model_70, "MLP_model_70.h5")
save_model(MLP_model_40, "MLP_model_40.h5")
save_model(MLP_model_10, "MLP_model_10.h5")

targets_70 = y_70_test_new
targets_40 = y_40_test_new
targets_10 = y_10_test_new

print('10E 70N (2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70, y_pred_70 )),2))
print('ANN Corr.:', scipy.stats.pearsonr(targets_70, y_pred_70)[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_70, y_pred_70 ),2))

print('10E 40N (2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40, y_pred_40 )),2))
print('ANN Corr.:',  scipy.stats.pearsonr(targets_40, y_pred_40 )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_40, y_pred_40 ),2))

#new
print('10E 10N (2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10,  y_pred_10 )),2))
print('ANN Corr.:',  scipy.stats.pearsonr(targets_10, y_pred_10 )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_10,  y_pred_10 ),2))

print('10E 70N (6-10 Sept 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[5928:6048], y_pred_70[5928:6048] )),2))
print('ANN  Corr.:',  scipy.stats.pearsonr(targets_70[5928:6048], y_pred_70[5928:6048] )[0])
print('ANN  MAE:',  round(metrics.mean_absolute_error(targets_70[5928:6048], y_pred_70[5928:6048]),2))

print('10E 40N (6-10 Sept 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[5928:6048], y_pred_40[5928:6048] )),2))
print('ANN Corr.:',  scipy.stats.pearsonr(targets_40[5928:6048], y_pred_40[5928:6048] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_40[5928:6048], y_pred_40[5928:6048]),2))

print('10E 10N (6-10 Sept 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[5928:6048],  y_pred_10[5928:6048] )),2))
print('ANN Corr.:',  scipy.stats.pearsonr(targets_10[5928:6048],  y_pred_10[5928:6048] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_10[5928:6048],  y_pred_10[5928:6048]),2))

#quiet period
print('10E 70N (25-29 April 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[2712:2832], y_pred_70[2712:2832] )),2))
print('ANN Corr.:', scipy.stats.pearsonr(targets_70[2712:2832], y_pred_70[2712:2832] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_70[2712:2832], y_pred_70[2712:2832] ),2))

print('10E 40N (25-29 April 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[2712:2832], y_pred_40[2712:2832] )),2))
print('ANN Corr.:', scipy.stats.pearsonr(targets_40[2712:2832], y_pred_40[2712:2832] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_40[2712:2832], y_pred_40[2712:2832] ),2))

print('10E 10N (25-29 April 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[2712:2832], y_pred_10[2712:2832] )),2))
print('ANN Corr.:', scipy.stats.pearsonr(targets_10[2712:2832], y_pred_10[2712:2832] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_10[2712:2832], y_pred_10[2712:2832] ),2))

"""## Bayesian neural network (BNN)

Bayesian Neural Network (BNN) VTEC model will be built. 

We define the prior weight distribution as a Gaussian distribution with a mean
μ = 0 and a diagonal covariance with a standard deviation σ = 1. A sample of the
weights w is obtained by randomly sampling ε from N (0, I), then scaling it by a standard deviation σ, and shifting it by a mean μ ([Bayes by Backprop](https://arxiv.org/abs/1505.05424) paper):

1. Sample $\epsilon ∼ N(0, I)$.
2. Let $w = µ + log(1 + exp(ρ)) ◦ \epsilon$.
3. Let $θ = (µ, ρ)$.
4. Let $f(w, θ) = KL(q∥p)$.
5. Compute gradients with respect to μ and σ  to update the previous distribution parameters θ using the stochastic gradient descent (SGD) optimization
algorithm .

Kullback-Leibler (KL) divergence is used to measure the dissimilarity of the variational (predicted) probability distribution q(x) to the true posterior probability distribution p(x). The reverse KL divergence KL(q∥p) is minimized in variational inference.

The gradients are calculated during backpropagation of a neural network. The parameters are updated stepwise, controlled by the learning rate, along a preferred direction, which is a function of the previous gradient. Each
time the model is run with the same input, it produces a different output results, because a new set of weights is sampled from the distribution each time. The final VTEC forecast is estimated as the mean of an ensemble of outputs, while the 95% confidence as 2 times standard deviations. For details check the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M. 

The `tfp.layers.DenseVariational` layer is used to build the BNN (see the [help page](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational) for more information). More complex posterior distributions is parametrized using `tfp.distributions.MultivariateNormalTriL` with complex covariance matrix.
"""

# Define the prior weight distribution as Normal of mean=0 and stddev=1.
def prior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    prior_model = keras.Sequential(
        [
            tfp.layers.DistributionLambda(
                lambda t: tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(n), 
                                                                   scale_diag=tf.ones(n))
            )
        ]
    )
    return prior_model


# Define variational posterior weight distribution as multivariate Gaussian.
# The learnable parameters for this distribution are the means, variances, and covariances.
def posterior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    posterior_model = keras.Sequential(
        [
            tfp.layers.VariableLayer(
                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype
            ),
            tfp.layers.MultivariateNormalTriL(n),
        ]
    )
    return posterior_model

def create_full_bnn_model(input_dim, hidden_units, kl_weight):
    inputs = layers.Input(shape=(input_dim,))
    features = layers.BatchNormalization()(inputs)

    # Create hidden layer with weight uncertainty using the DenseVariational layer
    for units in hidden_units:
        features = tfp.layers.DenseVariational(
            units=units,
            make_prior_fn=prior,
            make_posterior_fn=posterior,
            kl_weight=kl_weight,
            activation="sigmoid",
        )(features)

    outputs = tfp.layers.DenseVariational(
            units=1,
            make_prior_fn=prior,
            make_posterior_fn=posterior,
            kl_weight=kl_weight,
    )(features)

    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=20)

learning_rate = 0.001

def run_experiment(model, loss, num_epochs, num_batch_size, train_X, train_y):
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )
    count = 1
    xtrain_err = 0
    xval_err = 0
    splits=20 

    for train_index, test_index in tscv.split(train_X, train_y):
      X_train, X_val =train_X[train_index], train_X[test_index]
      y_train, y_val = train_y[train_index], train_y[test_index]
      
      history=model.fit(X_train, y_train, epochs=num_epochs, batch_size=num_batch, validation_data=(X_val, y_val))
      _, rmset = model.evaluate(X_train, y_train, verbose=0)
      print('Fold {}'.format(count))
      print(f"Train RMSE: {round(rmset, 3)}")
      
      _, rmsev = model.evaluate(X_val, y_val, verbose=0)
      print(f"Valid RMSE: {round(rmsev, 3)}")

      xtrain_err+=rmset
      xval_err+=rmsev
      
      count = count + 1
    
    train_rmse = xtrain_err/splits
    val_rmse = xval_err/splits
    print ('Average RMSE on train data', round(train_rmse,3))
    print ('Average RMSE on val data', round(val_rmse,3))

bnn_model_full_lowVTEC = create_full_bnn_model(16, [32], 1/train_X.size)
bnn_model_full_lowVTEC.summary()

bnn_model_full_midVTEC = create_full_bnn_model(16, [32], 1/train_X.size)
bnn_model_full_midVTEC.summary()

bnn_model_full_highVTEC = create_full_bnn_model(16, [32], 1/train_X.size)
bnn_model_full_highVTEC.summary()

mse_loss = keras.losses.MeanSquaredError()

num_epochs = 1000
num_batch= 500

import time
start = time.time()

run_experiment(bnn_model_full_lowVTEC, mse_loss, num_epochs, num_batch, train_X_10_scaled, train_y_10)

end = time.time()
print(end - start, "seconds")

num_epochs = 1000
num_batch= 500

import time
start = time.time()

run_experiment(bnn_model_full_midVTEC, mse_loss, num_epochs, num_batch, train_X_40_scaled, train_y_40)

end = time.time()
print(end - start, "seconds")

num_epochs = 1000
num_batch= 500

import time
start = time.time()

run_experiment(bnn_model_full_highVTEC, mse_loss, num_epochs, num_batch, train_X_70_scaled, train_y_70)

end = time.time()
print(end - start, "seconds")

"""### **Testing**"""

def compute_predictions(model, examples, iterations=100):
    predicted = []
    for _ in range(iterations):
        predicted.append(model(examples).numpy())
    predicted = np.concatenate(predicted, axis=1)
    return predicted

def display_predictions(predictions, targets):
    prediction_mean = np.mean(predictions, axis=1).tolist()
    prediction_min = np.min(predictions, axis=1).tolist()
    prediction_max = np.max(predictions, axis=1).tolist()
    prediction_range = (np.max(predictions, axis=1) - np.min(predictions, axis=1)).tolist()

    for idx in range(samples):
        print(
            f"Predictions mean: {round(prediction_mean[idx], 2)}, "
            f"min: {round(prediction_min[idx], 2)}, "
            f"max: {round(prediction_max[idx], 2)}, "
            f"range: {round(prediction_range[idx], 2)} - "
            f"Actual: {targets[idx]}"
        )

"""Predictions for the first 10 samples:"""

samples = 10

predictions_bnn1_40 = compute_predictions(bnn_model_full_midVTEC, test_X_40_scaled)
display_predictions(predictions_bnn1_40, y_40_test_new)

predictions_bnn1_70 = compute_predictions(bnn_model_full_highVTEC, test_X_70_scaled)
display_predictions(predictions_bnn1_70, y_70_test_new)

predictions_bnn1_10 = compute_predictions(bnn_model_full_lowVTEC, test_X_10_scaled)

prediction_mean_10 = np.mean(predictions_bnn1_10, axis=1).tolist()
prediction_min_10 = np.min(predictions_bnn1_10, axis=1).tolist()
prediction_max_10 = np.max(predictions_bnn1_10, axis=1).tolist()
prediction_range_10 = (np.max(predictions_bnn1_10, axis=1) - np.min(predictions_bnn1_10, axis=1)).tolist()

prediction_mean_40 = np.mean(predictions_bnn1_40, axis=1).tolist()
prediction_min_40 = np.min(predictions_bnn1_40, axis=1).tolist()
prediction_max_40 = np.max(predictions_bnn1_40, axis=1).tolist()
prediction_range_40 = (np.max(predictions_bnn1_40, axis=1) - np.min(predictions_bnn1_40, axis=1)).tolist()

prediction_mean_70 = np.mean(predictions_bnn1_70, axis=1).tolist()
prediction_min_70 = np.min(predictions_bnn1_70, axis=1).tolist()
prediction_max_70 = np.max(predictions_bnn1_70, axis=1).tolist()
prediction_range_70 = (np.max(predictions_bnn1_70, axis=1) - np.min(predictions_bnn1_70, axis=1)).tolist()

#Calculation of 95% VTEC confidence intervals for point 10E 10N
prediction_std_10 = np.std(predictions_bnn1_10, axis=1, ddof=0)
upper_10 = (prediction_mean_10 + (2 * prediction_std_10))
lower_10 = (prediction_mean_10 - (2 * prediction_std_10))

#Calculation of 95% VTEC confidence intervals for point 10E 40N
prediction_std_40 = np.std(predictions_bnn1_40, axis=1, ddof=0)
upper_40 = (prediction_mean_40 + (2 * prediction_std_40))
lower_40 = (prediction_mean_40 - (2 * prediction_std_40))

#Calculation of 95% VTEC confidence intervals for point 10E 70N
prediction_std_70 = np.std(predictions_bnn1_70, axis=1, ddof=0)
upper_70 = (prediction_mean_70 + (2 * prediction_std_70))
lower_70 = (prediction_mean_70 - (2 * prediction_std_70))

"""### **In-out (percentage)**

This calculates how much actual data (ground truth) lies within and outside the confidence interval in percent for the test data set.
"""

#Calculation of percentage of ground truth (GT) within CI
def compute_percentage_in_CI (y_test, y_pred_upper, y_pred_lower):
  diff_up = y_pred_upper - y_test
  diff_low = y_pred_lower - y_test
  
  in_values=0; out_values=0
  
  for i in range(diff_up.size):
    if np.any((diff_up[i] >= 0) & (diff_low[i] <= 0)):
      in_values = in_values + 1
    else:
      out_values = out_values + 1
        
  whole = in_values + out_values
  
  percent_in_CI = in_values/whole * 100
  percent_out_CI = out_values/whole * 100

  print('GT  within the CI (%):', round(percent_in_CI,2))
  print('GT outside the CI (%):', round(percent_out_CI,2))

print('10E 70N (2017)' )
compute_percentage_in_CI (targets_70, upper_70, lower_70)
print('---------' )
print('10E 40N (2017)' )
compute_percentage_in_CI (targets_40, upper_40, lower_40)
print('---------' )
print('10E 10N (2017)' )
compute_percentage_in_CI (targets_10, upper_10 , lower_10)

print('10E 70N (Apr 25-29, 2017)' )
compute_percentage_in_CI (targets_70[2712:2832], upper_70[2712:2832], lower_70[2712:2832]
print('---------' )
print('10E 40N (Apr 25-29, 2017)' )
compute_percentage_in_CI (targets_40[2712:2832], upper_40[2712:2832], lower_40[2712:2832]
print('---------' )
print('10E 10N (Apr 25-29, 2017)' )
compute_percentage_in_CI (targets_10[2712:2832], upper_10[2712:2832], lower_10[2712:2832])

print('10E 70N (Sep 6-10, 2017)' )
compute_percentage_in_CI (targets_70[5927:6047], upper_70[5927:6047], lower_70[5927:6047])
print('---------' )
print('10E 40N (Sep 6-10, 2017)' )
compute_percentage_in_CI (targets_40[5927:6047], upper_40[5927:6047], lower_40[5927:6047])
print('---------' )
print('10E 10N (Sep 6-10, 2017)' )
compute_percentage_in_CI (targets_10[5927:6047], upper_10[5927:6047], lower_10[5927:6047])

"""### **Figures**"""

from matplotlib.dates import DateFormatter
base = datetime.datetime(2017, 9, 6)
dates = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

base = datetime.datetime(2017, 4, 25)
dates2 = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

from matplotlib.dates import DateFormatter
fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates, data_BNN1_70_2017['prediction_mean_70'][5927:6047], 'bo',linewidth=1,markersize=2, label='BNN MSE')
axs[0].fill_between(dates,data_BNN1_70_2017['prediction_mean_70'] [5927:6047]- 2*data_BNN1_70_2017['prediction_std_70'][5927:6047], data_BNN1_70_2017['prediction_mean_70'] [5927:6047] + 2*data_BNN1_70_2017['prediction_std_70'][5927:6047],  alpha=0.4, color='tab:green', label='Uncertainty')
axs[0].plot(dates, data_BNN1_70_2017['targets_70'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 22.0, 10.0)))

axs[1].plot(dates, data_BNN1_40_2017['prediction_mean_40'][5927:6047], 'bo',linewidth=1,markersize=2, label='SE')
axs[1].fill_between(dates,data_BNN1_40_2017['prediction_mean_40'] [5927:6047]- 2*data_BNN1_40_2017['prediction_std_40'][5927:6047], data_BNN1_40_2017['prediction_mean_40'] [5927:6047] + 2*data_BNN1_40_2017['prediction_std_40'][5927:6047],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[1].plot(dates, data_BNN1_40_2017['targets_40'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 32.0, 10.0)))

axs[2].plot(dates, data_BNN1_10_2017['prediction_mean_10'][5927:6047], 'bo',linewidth=1,markersize=2, label='SE')
axs[2].fill_between(dates,data_BNN1_10_2017['prediction_mean_10'] [5927:6047]- 2*data_BNN1_10_2017['prediction_std_10'][5927:6047], data_BNN1_10_2017['prediction_mean_10'] [5927:6047] + 2*data_BNN1_10_2017['prediction_std_10'][5927:6047],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[2].plot(dates, data_BNN1_10_2017['targets_10'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 52.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()
plt.rcParams.update({'font.size': 15})

from matplotlib.dates import DateFormatter
fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates2, data_BNN1_70_2017['prediction_mean_70'][2712:2832], 'bo',linewidth=1,markersize=2, label='BNN MSE')
axs[0].fill_between(dates2,data_BNN1_70_2017['prediction_mean_70'] [2712:2832]- 2*data_BNN1_70_2017['prediction_std_70'][2712:2832], data_BNN1_70_2017['prediction_mean_70'][2712:2832] + 2*data_BNN1_70_2017['prediction_std_70'][2712:2832],  alpha=0.4, color='tab:green', label='Uncertainty')
axs[0].plot(dates2, data_BNN1_70_2017['targets_70'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 22.0, 10.0)))

axs[1].plot(dates2, data_BNN1_40_2017['prediction_mean_40'][2712:2832], 'bo',linewidth=1,markersize=2, label='SE')
axs[1].fill_between(dates2,data_BNN1_40_2017['prediction_mean_40'] [2712:2832]- 2*data_BNN1_40_2017['prediction_std_40'][2712:2832], data_BNN1_40_2017['prediction_mean_40'] [2712:2832] + 2*data_BNN1_40_2017['prediction_std_40'][2712:2832],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[1].plot(dates2, data_BNN1_40_2017['targets_40'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 32.0, 10.0)))

axs[2].plot(dates2, data_BNN1_10_2017['prediction_mean_10'][2712:2832], 'bo',linewidth=1,markersize=2, label='SE')
axs[2].fill_between(dates2,data_BNN1_10_2017['prediction_mean_10'] [2712:2832] - 2*data_BNN1_10_2017['prediction_std_10'][2712:2832], data_BNN1_10_2017['prediction_mean_10'] [2712:2832] + 2*data_BNN1_10_2017['prediction_std_10'][2712:2832],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[2].plot(dates2, data_BNN1_10_2017['targets_10'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 52.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()
plt.rcParams.update({'font.size': 15})

"""### **Statistics**"""

import time
start = time.time()

y_pred = bnn_model_full_lowVTEC.predict(test_X)

end = time.time()
print(end - start, "seconds")

print('10E 70N (2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70, prediction_mean_70 )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_70, prediction_mean_70 )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70, prediction_mean_70 ),2))

print('Max std:', round(prediction_std_70.max(axis=0), 2))
print('Min std:', round(prediction_std_70.min(axis=0),2))
print('Mean std:', round(prediction_std_70.mean(axis=0), 2))

print('10E 10N (2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10, prediction_mean_10 )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_10, prediction_mean_10 )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10, prediction_mean_10 ),2))

print('Max std:', round(prediction_std_10.max(axis=0), 2))
print('Min std:', round(prediction_std_10.min(axis=0),2))
print('Mean std:', round(prediction_std_10.mean(axis=0), 2))

print('10E 70N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[5928:6048], prediction_mean_70[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_70[5928:6048], prediction_mean_70[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70[5928:6048], prediction_mean_70[5928:6048]),2))

print('Max std:', round(prediction_std_70[5928:6048].max(axis=0), 2))
print('Min std:', round(prediction_std_70[5928:6048].min(axis=0),2))
print('Mean std:', round(prediction_std_70[5928:6048].mean(axis=0), 2))

print('10E 40N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[5928:6048], prediction_mean_40[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_40[5928:6048], prediction_mean_40[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_40[5928:6048], prediction_mean_40[5928:6048]),2))

print('Max std:', round(prediction_std_40[5928:6048].max(axis=0), 2))
print('Min std:', round(prediction_std_40[5928:6048].min(axis=0),2))
print('Mean std:', round(prediction_std_40[5928:6048].mean(axis=0), 2))

print('10E 10N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[5928:6048], prediction_mean_10[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_10[5928:6048], prediction_mean_10[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10[5928:6048], prediction_mean_10[5928:6048]),2))

print('Max std:', round(prediction_std_10[5928:6048].max(axis=0), 2))
print('Min std:', round(prediction_std_10[5928:6048].min(axis=0),2))
print('Mean std:', round(prediction_std_10[5928:6048].mean(axis=0), 2))

#quiet period
print('10E 70N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[2712:2832], prediction_mean_70[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_70[2712:2832], prediction_mean_70[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70[2712:2832], prediction_mean_40[2712:2832] ),2))

print('Max std:', round(prediction_std_70[2712:2832].max(axis=0), 2))
print('Min std:', round(prediction_std_70[2712:2832].min(axis=0),2))
print('Mean std:', round(prediction_std_70[2712:2832].mean(axis=0), 2))

print('10E 40N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[2712:2832], prediction_mean_40[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_40[2712:2832], prediction_mean_40[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_40[2712:2832], prediction_mean_40[2712:2832] ),2))

print('Max std:', round(prediction_std_40[2712:2832].max(axis=0), 2))
print('Min std:', round(prediction_std_40[2712:2832].min(axis=0),2))
print('Mean std:', round(prediction_std_40[2712:2832].mean(axis=0), 2))

print('10E 10N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[2712:2832], prediction_mean_10[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_10[2712:2832], prediction_mean_10[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10[2712:2832], prediction_mean_10[2712:2832] ),2))

print('Max std:', round(prediction_std_10[2712:2832].max(axis=0), 2))
print('Min std:', round(prediction_std_10[2712:2832].min(axis=0),2))
print('Mean std:', round(prediction_std_10[2712:2832].mean(axis=0), 2))

"""## Bayesian Neural Network with NLL loss

In the previous model, distributions were modeled over weights, but individual point estimates (VTEC) was provided for each input.
We can also model an output distribution, keeping the learning distribution over the weights as well. The last layer will be changed to parameterize the output. The negative log-likelihood (NLL) of this distribution is used as the loss function instead of the MSE. The NLL loss is also known as the negative logarithm of predictive density (NLPD). This approach is intended to model *aleatory uncertainty (data uncertainty)*, the irreducible noise in the data, and thus capture the stochastic nature of the process that generates the data.
"""

learning_rate = 0.01

def run_experiment(model, loss, num_epochs, num_batch_size, train_X, train_y):
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )
    count = 1
    xtrain_err = 0
    xval_err = 0
    splits=20 

    for train_index, test_index in tscv.split(train_X, train_y):
      X_train, X_val =train_X[train_index], train_X[test_index]
      y_train, y_val = train_y[train_index], train_y[test_index]
      
      history=model.fit(X_train, y_train, epochs=num_epochs, batch_size=num_batch, validation_data=(X_val, y_val))
      _, rmset = model.evaluate(X_train, y_train, verbose=0)
      print('Fold {}'.format(count))
      print(f"Train RMSE: {round(rmset, 3)}")
      
      _, rmsev = model.evaluate(X_val, y_val, verbose=0)
      print(f"Valid RMSE: {round(rmsev, 3)}")

      xtrain_err+=rmset
      xval_err+=rmsev
      
      count = count + 1
    
    train_rmse = xtrain_err/splits
    val_rmse = xval_err/splits
    print ('Average RMSE on train data', round(train_rmse,3))
    print ('Average RMSE on val data', round(val_rmse,3))

learning_rate = 0.1

def run_experiment(model, loss, num_epochs, num_batch_size, train_X, train_y):
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )
    count = 1
    xtrain_err = 0
    xval_err = 0
    splits=20 

    for train_index, test_index in tscv.split(train_X, train_y):
      X_train, X_val =train_X[train_index], train_X[test_index]
      y_train, y_val = train_y[train_index], train_y[test_index]
      
      history=model.fit(X_train, y_train, epochs=num_epochs, batch_size=num_batch, validation_data=(X_val, y_val))
      _, rmset = model.evaluate(X_train, y_train, verbose=0)
      print('Fold {}'.format(count))
      print(f"Train RMSE: {round(rmset, 3)}")
      
      _, rmsev = model.evaluate(X_val, y_val, verbose=0)
      print(f"Valid RMSE: {round(rmsev, 3)}")

      xtrain_err+=rmset
      xval_err+=rmsev
      
      count = count + 1
    
    train_rmse = xtrain_err/splits
    val_rmse = xval_err/splits
    print ('Average RMSE on train data', round(train_rmse,3))
    print ('Average RMSE on val data', round(val_rmse,3))

"""To provide the mean and variance as output of the model,  a custom output layer with two neurons will be created: for the mean VTEC output and for the standard deviation.  """

def create_probabilistic_bnn_model(input_dim, hidden_units, kl_weight):
    inputs = layers.Input(shape=(input_dim,))
    features = layers.BatchNormalization()(inputs)

    # Create hidden layers with weight uncertainty using the DenseVariational layer.
    for units in hidden_units:
        features = tfp.layers.DenseVariational(
            units=units,
            make_prior_fn=prior,
            make_posterior_fn=posterior,
            kl_weight=kl_weight,
            activation="sigmoid",
        )(features)

    distribution_params = layers.Dense(units=2)(features)
    outputs = tfp.layers.IndependentNormal(1)(distribution_params)

    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

def negative_loglikelihood(targets, estimated_distribution):
    return -estimated_distribution.log_prob(targets)

bnn_model_probabilistic_lowVTEC = create_probabilistic_bnn_model(16, [32], 1/train_X.size)
bnn_model_probabilistic_lowVTEC.summary()

bnn_model_probabilistic_midVTEC = create_probabilistic_bnn_model(16, [32], 1/train_X.size)
bnn_model_probabilistic_midVTEC.summary()

bnn_model_probabilistic_highVTEC = create_probabilistic_bnn_model(16, [32], 1/train_X.size)
bnn_model_probabilistic_highVTEC.summary()

num_epochs= 2000
num_batch= 500
#Learning_rate = 0.1

import time
start = time.time()

run_experiment(bnn_model_probabilistic_lowVTEC, negative_loglikelihood, num_epochs,num_batch,  train_X, train_y)

end = time.time()
print(end - start, "seconds")

num_epochs = 2000
num_batch= 500
#Learning_rate = 0.01

import time
start = time.time()

run_experiment(bnn_model_probabilistic_midVTEC, negative_loglikelihood, num_epochs,num_batch,  train_X, train_y)

end = time.time()
print(end - start, "seconds")

#Learning_rate=0.01
num_epochs =2000
num_batch= 500

import time
start = time.time()

run_experiment(bnn_model_probabilistic_highVTEC, negative_loglikelihood, num_epochs,num_batch,  train_X, train_y)

end = time.time()
print(end - start, "seconds")

"""### **Testing**"""

samples=10

prediction_distribution_10 = bnn_model_probabilistic_lowVTEC(test_X_10_scaled)
prediction_mean_aleatoric_10 = prediction_distribution_10.mean().numpy().tolist()
prediction_aleatoric_std_10 = prediction_distribution_10.stddev().numpy() 

# The 95% CI is computed as mean ± (2 * stdv)
upper_aleatoric_10 = (prediction_mean_aleatoric_10 + (2 * prediction_aleatoric_std_10)).tolist()
lower_aleatoric_10 = (prediction_mean_aleatoric_10 - (2 * prediction_aleatoric_std_10)).tolist()
prediction_aleatoric_stdv_10 = prediction_aleatoric_std_10.tolist()

for idx in range(samples):
    print(
        f"Prediction mean: {round(prediction_mean_aleatoric_10[idx][0], 2)}, "
        f"stddev: {round(prediction_aleatoric_stdv_10[idx][0], 2)}, "
        f"95% CI: [{round(upper_aleatoric_10[idx][0], 2)} - {round(lower_aleatoric_10[idx][0], 2)}]"
        f" - Actual: {y_10_test_new[idx]}"
    )

upper_d=np.array(upper_aleatoric_10)
lower_d=np.array(lower_aleatoric_10)
upperr_10 = upper_d.flatten()
lowerr_10 = lower_d.flatten()

prediction_distribution_40 = bnn_model_probabilistic_midVTEC(test_X_40_scaled)
prediction_mean_aleatoric_40 = prediction_distribution_40.mean().numpy().tolist()
prediction_aleatoric_std_40 = prediction_distribution_40.stddev().numpy() 

# The 95% CI is computed as mean ± (2 * stdv)
upper_aleatoric_40 = (prediction_mean_aleatoric_40 + (2 * prediction_aleatoric_std_40)).tolist()
lower_aleatoric_40 = (prediction_mean_aleatoric_40 - (2 * prediction_aleatoric_std_40)).tolist()
prediction_aleatoric_stdv_40 = prediction_aleatoric_std_40.tolist()

for idx in range(samples):
    print(
        f"Prediction mean: {round(prediction_mean_aleatoric_40[idx][0], 2)}, "
        f"stddev: {round(prediction_aleatoric_stdv_40[idx][0], 2)}, "
        f"95% CI: [{round(upper_aleatoric_40[idx][0], 2)} - {round(lower_aleatoric_40[idx][0], 2)}]"
        f" - Actual: {y_40_test_new[idx]}"
    )

upper_d=np.array(upper_aleatoric_40)
lower_d=np.array(lower_aleatoric_40)
upperr_40 = upper_d.flatten()
lowerr_40 = lower_d.flatten()

prediction_distribution_70 = bnn_model_probabilistic_highVTEC(test_X_70_scaled)
prediction_mean_aleatoric_70 = prediction_distribution_70.mean().numpy().tolist()
prediction_aleatoric_std_70 = prediction_distribution_70.stddev().numpy() 

# The 95% CI is computed as mean ± (2 * stdv)
upper_aleatoric_70 = (prediction_mean_aleatoric_70 + (2 * prediction_aleatoric_std_70)).tolist()
lower_aleatoric_70 = (prediction_mean_aleatoric_70 - (2 * prediction_aleatoric_std_70)).tolist()
prediction_aleatoric_stdv_70 = prediction_aleatoric_std_70.tolist()

for idx in range(samples):
    print(
        f"Prediction mean: {round(prediction_mean_aleatoric_70[idx][0], 2)}, "
        f"stddev: {round(prediction_aleatoric_stdv_70[idx][0], 2)}, "
        f"95% CI: [{round(upper_aleatoric_70[idx][0], 2)} - {round(lower_aleatoric_70[idx][0], 2)}]"
        f" - Actual: {y_70_test_new[idx]}"
    )

upper_d=np.array(upper_aleatoric_70)
lower_d=np.array(lower_aleatoric_70)
upperr_70 = upper_d.flatten()
lowerr_70 = lower_d.flatten()

"""### **In-out (percentage)**

This calculates how much actual data (ground truth) lies within and outside the confidence interval in percent for the test data set.
"""

#Calculation of percentage of ground truth (GT) within CI
def compute_percentage_in_CI (y_test, y_pred_upper, y_pred_lower):
  diff_up = y_pred_upper - y_test
  diff_low = y_pred_lower - y_test
  
  in_values=0; out_values=0
  
  for i in range(diff_up.size):
    if np.any((diff_up[i] >= 0) & (diff_low[i] <= 0)):
      in_values = in_values + 1
    else:
      out_values = out_values + 1
        
  whole = in_values + out_values
  
  percent_in_CI = in_values/whole * 100
  percent_out_CI = out_values/whole * 100

  print('GT  within the CI (%):', round(percent_in_CI,2))
  print('GT outside the CI (%):', round(percent_out_CI,2))

print('10E 70N (2017)' )
compute_percentage_in_CI (targets_70, upperr_70, lowerr_70)
print('---------' )
print('10E 40N (2017)' )
compute_percentage_in_CI (targets_40, upperr_40, lowerr_40)
print('---------' )
print('10E 10N (2017)' )
compute_percentage_in_CI (targets_10, upperr_10 , lowerr_10)

print('10E 70N (Apr 25-29, 2017)' )
compute_percentage_in_CI (targets_70[2712:2832], upperr_70[2712:2832], lowerr_70[2712:2832]
print('---------' )
print('10E 40N (Apr 25-29, 2017)' )
compute_percentage_in_CI (targets_40[2712:2832], upperr_40[2712:2832], lowerr_40[2712:2832]
print('---------' )
print('10E 10N (Apr 25-29, 2017)' )
compute_percentage_in_CI (targets_10[2712:2832], upperr_10[2712:2832], lowerr_10[2712:2832])

print('10E 70N (Sep 6-10, 2017)' )
compute_percentage_in_CI (targets_70[5927:6047], upperr_70[5927:6047], lowerr_70[5927:6047])
print('---------' )
print('10E 40N (Sep 6-10, 2017)' )
compute_percentage_in_CI (targets_40[5927:6047], upperr_40[5927:6047], lowerr_40[5927:6047])
print('---------' )
print('10E 10N (Sep 6-10, 2017)' )
compute_percentage_in_CI (targets_10[5927:6047], upperr_10[5927:6047], lowerr_10[5927:6047])

"""### **Figures**"""

from matplotlib.dates import DateFormatter
base = datetime.datetime(2017, 9, 6)
dates = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

plt.plot(dates,prediction_mean_aleatoric_10[5928:6048], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates,targets_10[5928:6048], 'o:',color='tab:orange', linewidth=1,markersize=4,  label='Ground truth')
plt.fill_between(dates, lowerr_10[5928:6048], upperr_10[5928:6048], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 62.0, 20.0)))
plt.rcParams ['figure.figsize'] = [10.0, 4.0]
plt.show()
plt.rcParams.update({'font.size': 16})

plt.plot(dates,prediction_mean_aleatoric_40[5928:6048], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates,targets_40[5928:6048], 'o:',color='tab:orange', linewidth=1,markersize=4,  label='Ground truth')
plt.fill_between(dates, lowerr_40[5928:6048], upperr_40[5928:6048], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 32.0, 15.0)))
plt.rcParams ['figure.figsize'] = [10.0, 4.0]
plt.show()
plt.rcParams.update({'font.size': 16})

plt.plot(dates,prediction_mean_aleatoric_70[5928:6048], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates,targets_70[5928:6048], 'o:',color='tab:orange', linewidth=1,markersize=4,  label='Ground truth')
plt.fill_between(dates, lowerr_70[5928:6048], upperr_70[5928:6048], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 22.0, 10.0)))
plt.rcParams ['figure.figsize'] = [10.0, 4.0]
plt.show()
plt.rcParams.update({'font.size': 16})

base = datetime.datetime(2017, 4, 25)
dates2 = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

plt.plot(dates2,prediction_mean_aleatoric_10[2712:2832], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates2, targets_10[2712:2832],  'o:',color='tab:orange', linewidth=1,markersize=4, label='Ground truth')
plt.fill_between(dates2, lowerr_10[2712:2832], upperr_10[2712:2832], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 62.0, 20.0)))
plt.show()
plt.rcParams.update({'font.size': 16})

plt.plot(dates2,prediction_mean_aleatoric_40[2712:2832], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates2, targets_40[2712:2832],  'o:',color='tab:orange', linewidth=1,markersize=4, label='Ground truth')
plt.fill_between(dates2, lowerr_40[2712:2832], upperr_40[2712:2832], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 32.0, 15.0)))
plt.show()
plt.rcParams.update({'font.size': 16})

plt.plot(dates2,prediction_mean_aleatoric_70[2712:2832], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates2, targets_70[2712:2832],  'o:',color='tab:orange', linewidth=1,markersize=4, label='Ground truth')
plt.fill_between(dates2, lowerr_70[2712:2832], upperr_70[2712:2832], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 22.0, 10.0)))
plt.show()
plt.rcParams.update({'font.size': 16})

fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates, data_BNN2_70_2017['prediction_mean_aleatoric_70'][5927:6047], 'bo',linewidth=1,markersize=2, label='BNN NLPD')
axs[0].fill_between(dates,data_BNN2_70_2017['prediction_mean_aleatoric_70'] [5927:6047]- 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][5927:6047], data_BNN2_70_2017['prediction_mean_aleatoric_70'] [5927:6047] + 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][5927:6047],  alpha=0.4, color='tab:green', label='Uncertainty')
axs[0].plot(dates, data_BNN2_70_2017['targets_70'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 22.0, 10.0)))

axs[1].plot(dates, data_BNN2_40_2017['prediction_aleatoric_mean_40'][5927:6047], 'bo',linewidth=1,markersize=2, label='SE')
axs[1].fill_between(dates,data_BNN2_40_2017['prediction_aleatoric_mean_40'] [5927:6047]- 2*data_BNN2_40_2017['prediction_aleatoric_std_40'][5927:6047], data_BNN2_40_2017['prediction_aleatoric_mean_40'] [5927:6047] + 2*data_BNN2_40_2017['prediction_aleatoric_std_40'][5927:6047],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[1].plot(dates, data_BNN2_40_2017['targets_40'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 32.0, 10.0)))

axs[2].plot(dates, data_BNN2_10_2017['prediction_mean_aleatoric_10'][5927:6047], 'bo',linewidth=1,markersize=2, label='SE')
axs[2].fill_between(dates,data_BNN2_10_2017['prediction_mean_aleatoric_10'] [5927:6047]- 2*data_BNN2_10_2017['prediction_aleatoric_std_10'][5927:6047], data_BNN2_10_2017['prediction_mean_aleatoric_10'] [5927:6047] + 2*data_BNN2_10_2017['prediction_aleatoric_std_10'][5927:6047],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[2].plot(dates, data_BNN2_10_2017['targets_10'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 52.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()
plt.rcParams.update({'font.size': 15})

fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates2, data_BNN2_70_2017['prediction_mean_aleatoric_70'][2712:2832], 'bo',linewidth=1,markersize=2, label='BNN NLPD')
axs[0].fill_between(dates2,data_BNN2_70_2017['prediction_mean_aleatoric_70'] [2712:2832]- 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][2712:2832], data_BNN2_70_2017['prediction_mean_aleatoric_70'][2712:2832] + 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][2712:2832],  alpha=0.4, color='tab:green', label='Uncertainty')
axs[0].plot(dates2, data_BNN2_70_2017['targets_70'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 22.0, 10.0)))

axs[1].plot(dates2, data_BNN2_40_2017['prediction_aleatoric_mean_40'][2712:2832], 'bo',linewidth=1,markersize=2, label='BNN NLPD')
axs[1].fill_between(dates2,data_BNN2_40_2017['prediction_aleatoric_mean_40'] [2712:2832]- 2*data_BNN2_40_2017['prediction_aleatoric_std_40'][2712:2832], data_BNN2_40_2017['prediction_aleatoric_mean_40'][2712:2832] + 2*data_BNN2_40_2017['prediction_aleatoric_std_40'][2712:2832],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[1].plot(dates2, data_BNN2_40_2017['targets_40'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 32.0, 10.0)))

axs[2].plot(dates2, data_BNN2_10_2017['prediction_mean_aleatoric_10'][2712:2832], 'bo',linewidth=1,markersize=2, label='BNN NLPD')
axs[2].fill_between(dates2,data_BNN2_10_2017['prediction_mean_aleatoric_10'][2712:2832]- 2*data_BNN2_10_2017['prediction_aleatoric_std_10'][2712:2832], data_BNN2_10_2017['prediction_mean_aleatoric_10'] [2712:2832] + 2*data_BNN2_10_2017['prediction_aleatoric_std_10'][2712:2832],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[2].plot(dates2, data_BNN2_10_2017['targets_10'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 52.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0,6.0]
plt.show()

"""The plots below show a comparison of the mean VTEC forecast by the Super Ensemble (SE) model, the BNN model, and the BNN-NLL model. For further analysis and plots, see the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M."""

#April 25-29, 2017.
fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates2, diff_70_apr_SE,linewidth=1,markersize=2, label='SE')
axs[0].plot(dates2, diff_70_apr_BNN1,linewidth=1,markersize=2, label='BNN MSE')
axs[0].plot(dates2, diff_70_apr,linewidth=1,markersize=2, label='BNN NLPD')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(-5, 6.0, 5.0)))

axs[1].plot(dates2, diff_40_apr_SE, linewidth=1,markersize=2, label='SE')
axs[1].plot(dates2, diff_40_apr_BNN1, linewidth=1,markersize=2, label='BNN MSE')
axs[1].plot(dates2, diff_40_apr,linewidth=1,markersize=2, label='BNN NLPD')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(-10, 11.0, 5.0)))

axs[2].plot(dates2, diff_10_apr_SE, linewidth=1,markersize=2, label='SE')
axs[2].plot(dates2, diff_10_apr_BNN1, linewidth=1,markersize=2, label='BNN MSE')
axs[2].plot(dates2, diff_10_apr,linewidth=1,markersize=2, label='BNN NLPD')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(-10, 11.0, 5.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()

#September 6-10, 2017. Note: incorrect dates in the figure.
fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates, diff_70_sept_SE,linewidth=1,markersize=2, label='SE')
axs[0].plot(dates, diff_70_sept_BNN1,linewidth=1,markersize=2, label='BNN MSE')
axs[0].plot(dates, diff_70_sept,linewidth=1,markersize=2, label='BNN NLPD')
#axs[0].fill_between(dates2,diff_70_apr- 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][2712:2832], diff_70_apr + 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][2712:2832],  alpha=0.4, color='tab:green', label='Uncertainty')
#axs[0].plot(dates2, data_BNN2_70_2017['targets_70'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(-5, 6.0, 5.0)))

axs[1].plot(dates, diff_40_sept_SE, linewidth=1,markersize=2, label='SE')
axs[1].plot(dates, diff_40_sept_BNN1, linewidth=1,markersize=2, label='BNN MSE')
axs[1].plot(dates, diff_40_sept, linewidth=1,markersize=2, label='BNN NLPD')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(-10, 11.0, 5.0)))

axs[2].plot(dates, diff_10_sept_SE, linewidth=1,markersize=2, label='SE')
axs[2].plot(dates, diff_10_sept_BNN1, linewidth=1,markersize=2, label='BNN MSE')
axs[2].plot(dates, diff_10_sept, linewidth=1,markersize=2, label='BNN NLPD')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(-10, 11.0, 5.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()

"""### **Statistics**"""

start = time.time()

y_pred = bnn_model_probabilistic_lowVTEC.predict(test_X)

end = time.time()
print(end - start, "seconds")

start = time.time()

y_pred = bnn_model_probabilistic_lowVTEC.predict(test_X)

end = time.time()
print(end - start, "seconds")

print('10E 70N (2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70, prediction_mean_aleatoric_70 )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_70, prediction_mean_aleatoric_70 )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70, prediction_mean_aleatoric_70 ),2))

print('Max std:', round(prediction_aleatoric_std_70.max(), 2))
print('Min std:', round(prediction_aleatoric_std_70.min(),2))
print('Mean std:', round(prediction_aleatoric_std_70.mean(), 2))
#NEW

print('10E 40N (2017)')
print('BNN2 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40, prediction_mean_aleatoric_40 )),2))
print('BNN2 Corr.:',  scipy.stats.pearsonr(targets_40, prediction_mean_aleatoric_40 )[0])
print('BNN2 MAE:',  round(metrics.mean_absolute_error(targets_40, prediction_mean_aleatoric_40 ),2))

print('Max std:', round(prediction_aleatoric_std_40.max(), 2))
print('Min std:', round(prediction_aleatoric_std_40.min(),2))
print('Mean std:', round(prediction_aleatoric_std_40.mean(), 2))

print('10E 10N (2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10, prediction_mean_aleatoric_10 )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_10, prediction_mean_aleatoric_10 )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10, prediction_mean_aleatoric_10 ),2))

print('Max std:', round(prediction_aleatoric_std_10.max(), 2))
print('Min std:', round(prediction_aleatoric_std_10.min(),2))
print('Mean std:', round(prediction_aleatoric_std_10.mean(), 2))

print('10E 70N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[5928:6048], prediction_mean_aleatoric_70[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_70[5928:6048], prediction_mean_aleatoric_70[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70[5928:6048], prediction_mean_aleatoric_70[5928:6048]),2))

print('Max std:', round(prediction_aleatoric_std_70[5928:6048].max(), 2))
print('Min std:', round(prediction_aleatoric_std_70[5928:6048].min(),2))
print('Mean std:', round(prediction_aleatoric_std_70[5928:6048].mean(), 2))

print('10E 40N (6-10 Sept 2017)')
print('BNN2 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[5928:6048], prediction_mean_aleatoric_40[5928:6048] )),2))
print('BNN2 Corr.:',  scipy.stats.pearsonr(targets_40[5928:6048], prediction_mean_aleatoric_40[5928:6048] )[0])
print('BNN2 MAE:',  round(metrics.mean_absolute_error(targets_40[5928:6048], prediction_mean_aleatoric_40[5928:6048]),2))

print('Max std:', round(prediction_aleatoric_std_40[5928:6048].max(), 2))
print('Min std:', round(prediction_aleatoric_std_40[5928:6048].min(),2))
print('Mean std:', round(prediction_aleatoric_std_40[5928:6048].mean(), 2))

print('10E 10N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[5928:6048], prediction_mean_aleatoric_10[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_10[5928:6048], prediction_mean_aleatoric_10[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10[5928:6048], prediction_mean_aleatoric_10[5928:6048]),2))

print('Max std:', round(prediction_aleatoric_std_10[5928:6048].max(), 2))
print('Min std:', round(prediction_aleatoric_std_10[5928:6048].min(),2))
print('Mean std:', round(prediction_aleatoric_std_10[5928:6048].mean(), 2))

#quiet period
print('10E 70N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[2712:2832], prediction_mean_aleatoric_70[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_70[2712:2832], prediction_mean_aleatoric_70[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70[2712:2832], prediction_mean_aleatoric_70[2712:2832] ),2))

print('Max std:', round(prediction_aleatoric_std_70[2712:2832].max(), 2))
print('Min std:', round(prediction_aleatoric_std_70[2712:2832].min(),2))
print('Mean std:', round(prediction_aleatoric_std_70[2712:2832].mean(), 2))

print('10E 40N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[2712:2832], prediction_mean_aleatoric_40[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_40[2712:2832], prediction_mean_aleatoric_40[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_40[2712:2832], prediction_mean_aleatoric_40[2712:2832] ),2))

print('Max std:', round(prediction_aleatoric_std_40[2712:2832].max(), 2))
print('Min std:', round(prediction_aleatoric_std_40[2712:2832].min(),2))
print('Mean std:', round(prediction_aleatoric_std_40[2712:2832].mean(), 2))

print('10E 10N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[2712:2832], prediction_mean_aleatoric_10[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_10[2712:2832], prediction_mean_aleatoric_10[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10[2712:2832], prediction_mean_aleatoric_10[2712:2832] ),2))

print('Max std:', round(prediction_aleatoric_std_10[2712:2832].max(), 2))
print('Min std:', round(prediction_aleatoric_std_10[2712:2832].min(),2))
print('Mean std:', round(prediction_aleatoric_std_10[2712:2832].mean(), 2))