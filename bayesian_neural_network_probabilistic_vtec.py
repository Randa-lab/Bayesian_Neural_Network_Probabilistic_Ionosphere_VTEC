# -*- coding: utf-8 -*-
"""Bayesian_Neural_Network_for_Probabilistic_VTEC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18_twiOz0naNhq-XLzCTdr0sLnrgRCF_M

## Installation

In this notebook, two Bayesian neural network models are developed for VTEC forecasting with 95% confidence intervals, from the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M., submitted to the Space Weather Jornal, AGU. In both models, the deterministic network parameters (weights) are replaced by probability distributions of these weights. However, the first model only describes the uncertainty in the weights and models the model uncertainty, while the second model also estimate the data uncertainty by providing probabilistic output estimate via the negative log-likelihood (NLL) loss.

The deep learning frameworks Tensorflow/Keras are considered with [TensorFlow Probability](https://www.tensorflow.org/probability) library.  The implementation is done in Google Colab. If you're outside of Colab, you may install it with pip:

```python
pip install tensorflow-probability
```

The notebook was created by Randa Natras: randa.natras@hotmail.com; randa.natras@tum.de

### **Libraries**
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_datasets as tfds
import tensorflow_probability as tfp

import numpy as np
import pandas as pd
import datetime
import matplotlib.dates as mdates
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats
from scipy import stats 
from sklearn import metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

!rm -rvf logs

logdir = "logs"

from tensorboard.plugins.hparams import api as hp

"""## The dataset

The dataset from the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M.
"""

data_df = pd.read_csv ('Training_data_VTEC.csv', delimiter=';')
data_test_df= pd.read_csv ('Test_data_VTEC.csv', delimiter=';')

data_df.head()

data_test_df.head()

data_df['Date-time'] = (pd.to_datetime(data_df['Year'] * 1000 + data_df['DOY'], format='%Y%j') + pd.to_timedelta(data_df['Hour '], unit='h'))
data_test_df['Date-time'] = (pd.to_datetime(data_test_df['Year'] * 1000 + data_test_df['DOY'], format='%Y%j') + pd.to_timedelta(data_test_df['Hour '], unit='h'))

data_df_2 = data_df.set_index('Date-time')
data_test_df_2 = data_test_df.set_index('Date-time')

data_df_2.head()

data_test_df_2.head()

"""### Create training and evaluation datasets"""

#Calculating the exponential moving average VTEC over 30 days.
data_df_2['VTEC_10N_EMA(30d)'] = data_df_2['VTEC_10N'].ewm(span=(30*24), adjust=False).mean()
data_df_2['VTEC_40N_EMA(30d)'] = data_df_2['VTEC_40N'].ewm(span=(30*24), adjust=False).mean()
data_df_2['VTEC_70N_EMA(30d)'] = data_df_2['VTEC_70N'].ewm(span=(30*24), adjust=False).mean()
data_test_df_2['VTEC_10N_EMA(30d)'] = data_test_df_2['VTEC_10N'].ewm(span=(30*24), adjust=False).mean()
data_test_df_2['VTEC_40N_EMA(30d)'] = data_test_df_2['VTEC_40N'].ewm(span=(30*24), adjust=False).mean()
data_test_df_2['VTEC_70N_EMA(30d)'] = data_test_df_2['VTEC_70N'].ewm(span=(30*24), adjust=False).mean()

#Calculating the exponential moving average VTEC over 4 days (96 hours).
data_df_2['VTEC_10N_EMA(96h)'] = data_df_2['VTEC_10N'].ewm(span=(4*24), adjust=False).mean()
data_df_2['VTEC_40N_EMA(96h)'] = data_df_2['VTEC_40N'].ewm(span=(4*24), adjust=False).mean()
data_df_2['VTEC_70N_EMA(96h)'] = data_df_2['VTEC_70N'].ewm(span=(4*24), adjust=False).mean()
data_test_df_2['VTEC_10N_EMA(96h)'] = data_test_df_2['VTEC_10N'].ewm(span=(4*24), adjust=False).mean()
data_test_df_2['VTEC_40N_EMA(96h)'] = data_test_df_2['VTEC_40N'].ewm(span=(4*24), adjust=False).mean()
data_test_df_2['VTEC_70N_EMA(96h)'] = data_test_df_2['VTEC_70N'].ewm(span=(4*24), adjust=False).mean()

#first order derivative
data_df_2['VTEC_10N_der1'] = data_df_2['VTEC_10N'].diff()
data_df_2['VTEC_40N_der1'] = data_df_2['VTEC_40N'].diff()
data_df_2['VTEC_70N_der1'] = data_df_2['VTEC_70N'].diff()
data_test_df_2['VTEC_10N_der1'] = data_test_df_2['VTEC_10N'].diff()
data_test_df_2['VTEC_40N_der1'] = data_test_df_2['VTEC_40N'].diff()
data_test_df_2['VTEC_70N_der1'] = data_test_df_2['VTEC_70N'].diff()

#second order derivative
data_df_2['VTEC_10N_der2'] = data_df_2['VTEC_10N'].diff().diff()
data_df_2['VTEC_40N_der2'] = data_df_2['VTEC_40N'].diff().diff()
data_df_2['VTEC_70N_der2'] = data_df_2['VTEC_70N'].diff().diff()
data_test_df_2['VTEC_10N_der2'] = data_test_df_2['VTEC_10N'].diff().diff()
data_test_df_2['VTEC_40N_der2'] = data_test_df_2['VTEC_40N'].diff().diff()
data_test_df_2['VTEC_70N_der2'] = data_test_df_2['VTEC_70N'].diff().diff()

#Calculation of the new features leads to NaN values, which are now omitted
data_df_2.dropna(inplace = True)
data_test_df_2.dropna(inplace = True)

data_df_2.head()

data_test_df_2.head()

"""Next, cyclic continuous features are used: Hour and DOY are transformed into 2 dimensions with sine and cosine.

"""

data_df_3=data_df_2.drop([ 'DOY', 'Hour '], axis=1)
data_test_df_3=data_test_df_2.drop([ 'DOY', 'Hour '], axis=1)

data_df_3['Hour (sin)'] = np.sin(2 * np.pi * data_df_2['Hour ']/24.0)
data_df_3['Hour (cos)'] = np.cos(2 * np.pi * data_df_2['Hour ']/24.0)
data_df_3['DOY (sin)'] = np.sin(2 * np.pi * data_df_2['DOY']/365.25)
data_df_3['DOY (cos)'] = np.cos(2 * np.pi * data_df_2['DOY']/365.25)

data_test_df_3['Hour (sin)'] = np.sin(2 * np.pi * data_test_df_2['Hour ']/24.0)
data_test_df_3['Hour (cos)'] = np.cos(2 * np.pi * data_test_df_2['Hour ']/24.0)
data_test_df_3['DOY (sin)'] = np.sin(2 * np.pi * data_test_df_2['DOY']/365.25)
data_test_df_3['DOY (cos)'] = np.cos(2 * np.pi * data_test_df_2['DOY']/365.25)

data_df_3.head()

data_test_df_3.head()

"""Preparation of the data for training and testing the model: extraction of the X and Y variables."""

X_10_df=data_df_3.drop (['Year','VTEC_40N', 'VTEC_70N','VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)',  'VTEC_40N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_70N_EMA(96h)', 'VTEC_40N_der1', 'VTEC_70N_der1', 'VTEC_40N_der2', 'VTEC_70N_der2'], axis=1)
X_40_df=data_df_3.drop (['Year','VTEC_10N', 'VTEC_70N','VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)', 'VTEC_10N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_10N_EMA(96h)', 'VTEC_70N_EMA(96h)', 'VTEC_10N_der1', 'VTEC_70N_der1', 'VTEC_10N_der2', 'VTEC_70N_der2' ], axis=1)
X_70_df=data_df_3.drop (['Year','VTEC_10N', 'VTEC_40N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)' ,  'VTEC_40N_EMA(30d)', 'VTEC_10N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_10N_EMA(96h)',  'VTEC_40N_der1', 'VTEC_10N_der1', 'VTEC_40N_der2', 'VTEC_10N_der2'], axis=1)
X_10_test_df=data_test_df_3.drop (['Year','VTEC_40N', 'VTEC_70N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)', 'VTEC_40N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_70N_EMA(96h)',  'VTEC_40N_der1', 'VTEC_70N_der1', 'VTEC_40N_der2', 'VTEC_70N_der2' ], axis=1)
X_40_test_df=data_test_df_3.drop (['Year','VTEC_10N', 'VTEC_70N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)' , 'VTEC_10N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_10N_EMA(96h)', 'VTEC_70N_EMA(96h)', 'VTEC_10N_der1', 'VTEC_70N_der1', 'VTEC_10N_der2', 'VTEC_70N_der2'], axis=1)
X_70_test_df=data_test_df_3.drop (['Year','VTEC_10N', 'VTEC_40N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)' , 'VTEC_40N_EMA(30d)', 'VTEC_10N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_10N_EMA(96h)', 'VTEC_40N_der1', 'VTEC_10N_der1', 'VTEC_40N_der2', 'VTEC_10N_der2'], axis=1)

X_10_df.head()

X_10_test_df.head()

X_10 = X_10_df.to_numpy()
X_40 = X_40_df.to_numpy()
X_70 = X_70_df.to_numpy()
X_10_test_new = X_10_test_df.to_numpy()
X_40_test_new = X_40_test_df.to_numpy()
X_70_test_new = X_70_test_df.to_numpy()

train_y_10 = data_df_3 ['VTEC_10N (t+24)'].to_numpy()
train_y_40 = data_df_3 ['VTEC_40N (t+24)'].to_numpy()
train_y_70 = data_df_3 ['VTEC_70N (t+24)'].to_numpy()
y_10_test_new = data_test_df_3 ['VTEC_10N (t+24)'].to_numpy()
y_40_test_new = data_test_df_3 ['VTEC_40N (t+24)'].to_numpy()
y_70_test_new = data_test_df_3 ['VTEC_70N (t+24)'].to_numpy()

#Data standardization
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler_70 = scaler.fit(X_70)
scaler_40= scaler.fit(X_40)
scaler_10 = scaler.fit(X_10)

train_X_70_scaled = scaler_70.transform(X_70)
train_X_40_scaled = scaler_40.transform(X_40)
train_X_10_scaled = scaler_10.transform(X_10)

test_X_70_scaled = scaler_70.transform(X_70_test_new)
test_X_40_scaled = scaler_40.transform(X_40_test_new)
test_X_10_scaled = scaler_10.transform(X_10_test_new)

"""## Standard artificial neural network

A standard deterministic artificial neural network model (MLP) is created as a baseline with the ANN architecture from the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M.: 1 hidden layer with 32 neurons.
"""

def create_1layer_model(input_dim, hidden_dim):
    inputs = layers.Input(shape=(input_dim,))

    # Hidden layer with deterministic weights using a Dense layer and non linear activation function
    features = layers.Dense(hidden_dim, activation="sigmoid")(inputs)

    # The output is deterministic: a single point estimate.
    outputs = layers.Dense(units=1)(features)

    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=20)

learning_rate = 0.001

def run_experiment(model, loss, num_epochs, num_batch_size, X_train, y_train):
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )
    
    history=model.fit(X_train, y_train, epochs=num_epochs, batch_size=num_batch)
    _, rmset = model.evaluate(X_train, y_train, verbose=0)
    print(f"Train RMSE: {round(rmset, 3)}")

MLP_model_10 = create_1layer_model(16,32)
MLP_model_10.summary()

MLP_model_40 = create_1layer_model(16,32)
MLP_model_70 = create_1layer_model(16,32)

#Model training and cross-validation for VTEC point located at 10E 10N
num_epochs = 1000
num_batch= 500
mse_loss = keras.losses.MeanSquaredError()
run_experiment(MLP_model_10, mse_loss, num_epochs, num_batch, train_X_10_scaled, train_y_10)

#Model training and cross-validation for VTEC point located at 10E 40N
num_epochs = 1000
num_batch= 500
mse_loss = keras.losses.MeanSquaredError()
run_experiment(MLP_model_40, mse_loss, num_epochs, num_batch, train_X_40_scaled, train_y_40)

#Model training and cross-validation for VTEC point located at 10E 70N
num_epochs = 1000
num_batch= 500
mse_loss = keras.losses.MeanSquaredError()
run_experiment(MLP_model_70, mse_loss, num_epochs, num_batch, train_X_70_scaled, train_y_70)

"""### **Testing**

We take the test set to obtain predictions for the previously unseen data.
Since the ANN model is deterministic, we get a single a
point estimate for each test example, with no information about the
uncertainty of the model nor the prediction.
"""

import time
start = time.time()

y_pred_10 = MLP_model_10.predict(test_X_10_scaled).flatten()
y_pred_40 = MLP_model_40.predict(test_X_40_scaled).flatten()
y_pred_70 = MLP_model_70.predict(test_X_70_scaled).flatten()

end = time.time()
print(end - start, "seconds")

from tensorflow.keras.models import save_model
save_model(MLP_model_70, "MLP_model_70.h5")
save_model(MLP_model_40, "MLP_model_40.h5")
save_model(MLP_model_10, "MLP_model_10.h5")

targets_70 = y_70_test_new
targets_40 = y_40_test_new
targets_10 = y_10_test_new

print('10E 70N (2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70, y_pred_70 )),2))
print('ANN Corr.:', scipy.stats.pearsonr(targets_70, y_pred_70)[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_70, y_pred_70 ),2))

print('10E 40N (2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40, y_pred_40 )),2))
print('ANN Corr.:',  scipy.stats.pearsonr(targets_40, y_pred_40 )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_40, y_pred_40 ),2))

#new
print('10E 10N (2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10,  y_pred_10 )),2))
print('ANN Corr.:',  scipy.stats.pearsonr(targets_10, y_pred_10 )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_10,  y_pred_10 ),2))

print('10E 70N (6-10 Sept 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[5928:6048], y_pred_70[5928:6048] )),2))
print('ANN  Corr.:',  scipy.stats.pearsonr(targets_70[5928:6048], y_pred_70[5928:6048] )[0])
print('ANN  MAE:',  round(metrics.mean_absolute_error(targets_70[5928:6048], y_pred_70[5928:6048]),2))

print('10E 40N (6-10 Sept 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[5928:6048], y_pred_40[5928:6048] )),2))
print('ANN Corr.:',  scipy.stats.pearsonr(targets_40[5928:6048], y_pred_40[5928:6048] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_40[5928:6048], y_pred_40[5928:6048]),2))

print('10E 10N (6-10 Sept 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[5928:6048],  y_pred_10[5928:6048] )),2))
print('ANN Corr.:',  scipy.stats.pearsonr(targets_10[5928:6048],  y_pred_10[5928:6048] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_10[5928:6048],  y_pred_10[5928:6048]),2))

#quiet period
print('10E 70N (25-29 April 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[2712:2832], y_pred_70[2712:2832] )),2))
print('ANN Corr.:', scipy.stats.pearsonr(targets_70[2712:2832], y_pred_70[2712:2832] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_70[2712:2832], y_pred_70[2712:2832] ),2))

print('10E 40N (25-29 April 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[2712:2832], y_pred_40[2712:2832] )),2))
print('ANN Corr.:', scipy.stats.pearsonr(targets_40[2712:2832], y_pred_40[2712:2832] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_40[2712:2832], y_pred_40[2712:2832] ),2))

print('10E 10N (25-29 April 2017)')
print('ANN RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[2712:2832], y_pred_10[2712:2832] )),2))
print('ANN Corr.:', scipy.stats.pearsonr(targets_10[2712:2832], y_pred_10[2712:2832] )[0])
print('ANN MAE:',  round(metrics.mean_absolute_error(targets_10[2712:2832], y_pred_10[2712:2832] ),2))

"""## Bayesian neural network (BNN)

Bayesian Neural Network (BNN) VTEC model will be built. 

We define the prior weight distribution as a Gaussian distribution with a mean
μ = 0 and a diagonal covariance with a standard deviation σ = 1. A sample of the
weights w is obtained by randomly sampling ε from N (0, I), then scaling it by a standard deviation σ, and shifting it by a mean μ ([Bayes by Backprop](https://arxiv.org/abs/1505.05424) paper):

1. Sample $\epsilon ∼ N(0, I)$.
2. Let $w = µ + log(1 + exp(ρ)) ◦ \epsilon$.
3. Let $θ = (µ, ρ)$.
4. Let $f(w, θ) = KL(q∥p)$.
5. Compute gradients with respect to μ and σ  to update the previous distribution parameters θ using the stochastic gradient descent (SGD) optimization
algorithm .

Kullback-Leibler (KL) divergence is used to measure the dissimilarity of the variational (predicted) probability distribution q(x) to the true posterior probability distribution p(x). The reverse KL divergence KL(q∥p) is minimized in variational inference.

The gradients are calculated during backpropagation of a neural network. The parameters are updated stepwise, controlled by the learning rate, along a preferred direction, which is a function of the previous gradient. Each
time the model is run with the same input, it produces a different output results, because a new set of weights is sampled from the distribution each time. The final VTEC forecast is estimated as the mean of an ensemble of outputs, while the 95% confidence as 2 times standard deviations. For details check the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M. 

The `tfp.layers.DenseVariational` layer is used to build the BNN (see the [help page](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational) for more information). More complex posterior distributions is parametrized using `tfp.distributions.MultivariateNormalTriL` with complex covariance matrix.
"""

# Define the prior weight distribution as Normal of mean=0 and stddev=1.
def prior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    prior_model = keras.Sequential(
        [
            tfp.layers.DistributionLambda(
                lambda t: tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(n), 
                                                                   scale_diag=tf.ones(n))
            )
        ]
    )
    return prior_model


# Define variational posterior weight distribution as multivariate Gaussian.
# The learnable parameters for this distribution are the means, variances, and covariances.
def posterior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    posterior_model = keras.Sequential(
        [
            tfp.layers.VariableLayer(
                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype
            ),
            tfp.layers.MultivariateNormalTriL(n),
        ]
    )
    return posterior_model

def create_full_bnn_model(input_dim, hidden_units, kl_weight):
    inputs = layers.Input(shape=(input_dim,))
    features = layers.BatchNormalization()(inputs)

    # Create hidden layer with weight uncertainty using the DenseVariational layer
    for units in hidden_units:
        features = tfp.layers.DenseVariational(
            units=units,
            make_prior_fn=prior,
            make_posterior_fn=posterior,
            kl_weight=kl_weight,
            activation="sigmoid",
        )(features)

    outputs = tfp.layers.DenseVariational(
            units=1,
            make_prior_fn=prior,
            make_posterior_fn=posterior,
            kl_weight=kl_weight,
    )(features)

    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=20)

learning_rate = 0.001

def run_experiment(model, loss, num_epochs, num_batch_size, train_X, train_y):
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )
    count = 1
    xtrain_err = 0
    xval_err = 0
    splits=20 

    for train_index, test_index in tscv.split(train_X, train_y):
      X_train, X_val =train_X[train_index], train_X[test_index]
      y_train, y_val = train_y[train_index], train_y[test_index]
      
      history=model.fit(X_train, y_train, epochs=num_epochs, batch_size=num_batch, validation_data=(X_val, y_val))
      _, rmset = model.evaluate(X_train, y_train, verbose=0)
      print('Fold {}'.format(count))
      print(f"Train RMSE: {round(rmset, 3)}")
      
      _, rmsev = model.evaluate(X_val, y_val, verbose=0)
      print(f"Valid RMSE: {round(rmsev, 3)}")

      xtrain_err+=rmset
      xval_err+=rmsev
      
      count = count + 1
    
    train_rmse = xtrain_err/splits
    val_rmse = xval_err/splits
    print ('Average RMSE on train data', round(train_rmse,3))
    print ('Average RMSE on val data', round(val_rmse,3))

bnn_model_full_lowVTEC = create_full_bnn_model(16, [32], 1/train_X.size)
bnn_model_full_lowVTEC.summary()

bnn_model_full_midVTEC = create_full_bnn_model(16, [32], 1/train_X.size)
bnn_model_full_midVTEC.summary()

bnn_model_full_highVTEC = create_full_bnn_model(16, [32], 1/train_X.size)
bnn_model_full_highVTEC.summary()

mse_loss = keras.losses.MeanSquaredError()

num_epochs = 1000
num_batch= 500

import time
start = time.time()

run_experiment(bnn_model_full_lowVTEC, mse_loss, num_epochs, num_batch, train_X_10_scaled, train_y_10)

end = time.time()
print(end - start, "seconds")

num_epochs = 1000
num_batch= 500

import time
start = time.time()

run_experiment(bnn_model_full_midVTEC, mse_loss, num_epochs, num_batch, train_X_40_scaled, train_y_40)

end = time.time()
print(end - start, "seconds")

num_epochs = 1000
num_batch= 500

import time
start = time.time()

run_experiment(bnn_model_full_highVTEC, mse_loss, num_epochs, num_batch, train_X_70_scaled, train_y_70)

end = time.time()
print(end - start, "seconds")

"""### **Testing**"""

def compute_predictions(model, examples, iterations=100):
    predicted = []
    for _ in range(iterations):
        predicted.append(model(examples).numpy())
    predicted = np.concatenate(predicted, axis=1)
    return predicted

def display_predictions(predictions, targets):
    prediction_mean = np.mean(predictions, axis=1).tolist()
    prediction_min = np.min(predictions, axis=1).tolist()
    prediction_max = np.max(predictions, axis=1).tolist()
    prediction_range = (np.max(predictions, axis=1) - np.min(predictions, axis=1)).tolist()

    for idx in range(samples):
        print(
            f"Predictions mean: {round(prediction_mean[idx], 2)}, "
            f"min: {round(prediction_min[idx], 2)}, "
            f"max: {round(prediction_max[idx], 2)}, "
            f"range: {round(prediction_range[idx], 2)} - "
            f"Actual: {targets[idx]}"
        )

"""Predictions for the first 10 samples:"""

samples = 10

predictions_bnn1_40 = compute_predictions(bnn_model_full_midVTEC, test_X_40_scaled)
display_predictions(predictions_bnn1_40, y_40_test_new)

predictions_bnn1_70 = compute_predictions(bnn_model_full_highVTEC, test_X_70_scaled)
display_predictions(predictions_bnn1_70, y_70_test_new)

predictions_bnn1_10 = compute_predictions(bnn_model_full_lowVTEC, test_X_10_scaled)

prediction_mean_10 = np.mean(predictions_bnn1_10, axis=1).tolist()
prediction_min_10 = np.min(predictions_bnn1_10, axis=1).tolist()
prediction_max_10 = np.max(predictions_bnn1_10, axis=1).tolist()
prediction_range_10 = (np.max(predictions_bnn1_10, axis=1) - np.min(predictions_bnn1_10, axis=1)).tolist()

prediction_mean_40 = np.mean(predictions_bnn1_40, axis=1).tolist()
prediction_min_40 = np.min(predictions_bnn1_40, axis=1).tolist()
prediction_max_40 = np.max(predictions_bnn1_40, axis=1).tolist()
prediction_range_40 = (np.max(predictions_bnn1_40, axis=1) - np.min(predictions_bnn1_40, axis=1)).tolist()

prediction_mean_70 = np.mean(predictions_bnn1_70, axis=1).tolist()
prediction_min_70 = np.min(predictions_bnn1_70, axis=1).tolist()
prediction_max_70 = np.max(predictions_bnn1_70, axis=1).tolist()
prediction_range_70 = (np.max(predictions_bnn1_70, axis=1) - np.min(predictions_bnn1_70, axis=1)).tolist()

#Calculation of 95% VTEC confidence intervals for point 10E 10N
prediction_std_10 = np.std(predictions_bnn1_10, axis=1, ddof=0)
upper_10 = (prediction_mean_10 + (2 * prediction_std_10))
lower_10 = (prediction_mean_10 - (2 * prediction_std_10))

#Calculation of 95% VTEC confidence intervals for point 10E 40N
prediction_std_40 = np.std(predictions_bnn1_40, axis=1, ddof=0)
upper_40 = (prediction_mean_40 + (2 * prediction_std_40))
lower_40 = (prediction_mean_40 - (2 * prediction_std_40))

#Calculation of 95% VTEC confidence intervals for point 10E 70N
prediction_std_70 = np.std(predictions_bnn1_70, axis=1, ddof=0)
upper_70 = (prediction_mean_70 + (2 * prediction_std_70))
lower_70 = (prediction_mean_70 - (2 * prediction_std_70))

"""### **In-out (percentage)**

This calculates how many data are inside and outside the confidence interval in percent for the test data set.
"""

#Test data of year 2017
diff_up_10 = upper_10 - targets_10
diff_low_10 = lower_10 - targets_10
diff_up_40 = upper_40 - targets_40
diff_low_40 = lower_40 - targets_40
diff_up_70 = upper_70 - targets_70
diff_low_70 = lower_70 - targets_70

#Test data September 6-10,2017
diff_up_10_sep = upper_10[5927:6047] - targets_10[5927:6047]
diff_low_10_sep = lower_10[5927:6047] - targets_10[5927:6047]
diff_up_40_sep = upper_40[5927:6047] - targets_40[5927:6047]
diff_low_40_sep = lower_40[5927:6047] - targets_40[5927:6047]
diff_up_70_sep = upper_70[5927:6047] - targets_70[5927:6047]
diff_low_70_sep = lower_70[5927:6047] - targets_70[5927:6047]

#Test data April 25-29,2017
diff_up_10_apr = upper_10[2712:2832] - targets_10[2712:2832]
diff_low_10_apr = lower_10[2712:2832] - targets_10[2712:2832]
diff_up_40_apr = upper_40[2712:2832] - targets_40[2712:2832]
diff_low_40_apr = lower_40[2712:2832]- targets_40[2712:2832]
diff_up_70_apr = upper_70[2712:2832] - targets_70[2712:2832]
diff_low_70_apr = lower_70[2712:2832] - targets_70[2712:2832]

diff_up_10_apr.size

data_2017_len=8758
data_apr_sep_len=120

a=0
b=0
for i in range(data_2017_len):
  if np.any((diff_up_10.values[i] >= 0) & (diff_low_10.values[i] <= 0)):
    a=a+1
  else:
    b=b+1

whole= a+b
percent_in_CI_10 = a/whole * 100
percent_out_CI_10 = b/whole * 100

#2017, 10E 10N
percent_in_CI_10

#September 6-10,2017, 10E 10N
percent_in_CI_10

#April 25-29,2017, 10E 10N
percent_in_CI_10

a=0
b=0
for i in range(data_apr_sep_len):
  if np.any((diff_up_40.values[i] >= 0) & (diff_low_40.values[i] <= 0)):
    a=a+1
  else:
    b=b+1

whole= a+b
percent_in_CI_40 = a/whole * 100
percent_out_CI_40 = b/whole * 100

#2017, 10E 40N
percent_in_CI_40

#September 6-10,2017, 10E 40N
percent_in_CI_40

#April 25-29,2017, 10E 40N
percent_in_CI_40

a=0
b=0
for i in range(data_apr_sep_len):
  if np.any((diff_up_70.values[i] >= 0) & (diff_low_70.values[i] <= 0)):
    a=a+1
  else:
    b=b+1

whole= a+b
percent_in_CI_70 = a/whole * 100
percent_out_CI_70 = b/whole * 100

#2017, 10E 70N
percent_in_CI_70

#September 6-10,2017, 10E 70N
percent_in_CI_70

#April 25-29,2017, 10E 70N
percent_in_CI_70

"""### **Figures**"""

from matplotlib.dates import DateFormatter
base = datetime.datetime(2017, 9, 6)
dates = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

base = datetime.datetime(2017, 4, 25)
dates2 = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

from matplotlib.dates import DateFormatter
fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates, data_BNN1_70_2017['prediction_mean_70'][5927:6047], 'bo',linewidth=1,markersize=2, label='BNN MSE')
axs[0].fill_between(dates,data_BNN1_70_2017['prediction_mean_70'] [5927:6047]- 2*data_BNN1_70_2017['prediction_std_70'][5927:6047], data_BNN1_70_2017['prediction_mean_70'] [5927:6047] + 2*data_BNN1_70_2017['prediction_std_70'][5927:6047],  alpha=0.4, color='tab:green', label='Uncertainty')
axs[0].plot(dates, data_BNN1_70_2017['targets_70'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 22.0, 10.0)))

axs[1].plot(dates, data_BNN1_40_2017['prediction_mean_40'][5927:6047], 'bo',linewidth=1,markersize=2, label='SE')
axs[1].fill_between(dates,data_BNN1_40_2017['prediction_mean_40'] [5927:6047]- 2*data_BNN1_40_2017['prediction_std_40'][5927:6047], data_BNN1_40_2017['prediction_mean_40'] [5927:6047] + 2*data_BNN1_40_2017['prediction_std_40'][5927:6047],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[1].plot(dates, data_BNN1_40_2017['targets_40'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 32.0, 10.0)))

axs[2].plot(dates, data_BNN1_10_2017['prediction_mean_10'][5927:6047], 'bo',linewidth=1,markersize=2, label='SE')
axs[2].fill_between(dates,data_BNN1_10_2017['prediction_mean_10'] [5927:6047]- 2*data_BNN1_10_2017['prediction_std_10'][5927:6047], data_BNN1_10_2017['prediction_mean_10'] [5927:6047] + 2*data_BNN1_10_2017['prediction_std_10'][5927:6047],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[2].plot(dates, data_BNN1_10_2017['targets_10'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 52.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()
plt.rcParams.update({'font.size': 15})

from matplotlib.dates import DateFormatter
fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates2, data_BNN1_70_2017['prediction_mean_70'][2712:2832], 'bo',linewidth=1,markersize=2, label='BNN MSE')
axs[0].fill_between(dates2,data_BNN1_70_2017['prediction_mean_70'] [2712:2832]- 2*data_BNN1_70_2017['prediction_std_70'][2712:2832], data_BNN1_70_2017['prediction_mean_70'][2712:2832] + 2*data_BNN1_70_2017['prediction_std_70'][2712:2832],  alpha=0.4, color='tab:green', label='Uncertainty')
axs[0].plot(dates2, data_BNN1_70_2017['targets_70'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 22.0, 10.0)))

axs[1].plot(dates2, data_BNN1_40_2017['prediction_mean_40'][2712:2832], 'bo',linewidth=1,markersize=2, label='SE')
axs[1].fill_between(dates2,data_BNN1_40_2017['prediction_mean_40'] [2712:2832]- 2*data_BNN1_40_2017['prediction_std_40'][2712:2832], data_BNN1_40_2017['prediction_mean_40'] [2712:2832] + 2*data_BNN1_40_2017['prediction_std_40'][2712:2832],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[1].plot(dates2, data_BNN1_40_2017['targets_40'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 32.0, 10.0)))

axs[2].plot(dates2, data_BNN1_10_2017['prediction_mean_10'][2712:2832], 'bo',linewidth=1,markersize=2, label='SE')
axs[2].fill_between(dates2,data_BNN1_10_2017['prediction_mean_10'] [2712:2832] - 2*data_BNN1_10_2017['prediction_std_10'][2712:2832], data_BNN1_10_2017['prediction_mean_10'] [2712:2832] + 2*data_BNN1_10_2017['prediction_std_10'][2712:2832],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[2].plot(dates2, data_BNN1_10_2017['targets_10'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 52.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()
plt.rcParams.update({'font.size': 15})

"""### **Statistics**"""

import time
start = time.time()

y_pred = bnn_model_full_lowVTEC.predict(test_X)

end = time.time()
print(end - start, "seconds")

print('10E 70N (2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70, prediction_mean_70 )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_70, prediction_mean_70 )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70, prediction_mean_70 ),2))

print('Max std:', round(prediction_std_70.max(axis=0), 2))
print('Min std:', round(prediction_std_70.min(axis=0),2))
print('Mean std:', round(prediction_std_70.mean(axis=0), 2))

print('10E 10N (2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10, prediction_mean_10 )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_10, prediction_mean_10 )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10, prediction_mean_10 ),2))

print('Max std:', round(prediction_std_10.max(axis=0), 2))
print('Min std:', round(prediction_std_10.min(axis=0),2))
print('Mean std:', round(prediction_std_10.mean(axis=0), 2))

print('10E 70N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[5928:6048], prediction_mean_70[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_70[5928:6048], prediction_mean_70[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70[5928:6048], prediction_mean_70[5928:6048]),2))

print('Max std:', round(prediction_std_70[5928:6048].max(axis=0), 2))
print('Min std:', round(prediction_std_70[5928:6048].min(axis=0),2))
print('Mean std:', round(prediction_std_70[5928:6048].mean(axis=0), 2))

print('10E 40N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[5928:6048], prediction_mean_40[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_40[5928:6048], prediction_mean_40[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_40[5928:6048], prediction_mean_40[5928:6048]),2))

print('Max std:', round(prediction_std_40[5928:6048].max(axis=0), 2))
print('Min std:', round(prediction_std_40[5928:6048].min(axis=0),2))
print('Mean std:', round(prediction_std_40[5928:6048].mean(axis=0), 2))

print('10E 10N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[5928:6048], prediction_mean_10[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_10[5928:6048], prediction_mean_10[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10[5928:6048], prediction_mean_10[5928:6048]),2))

print('Max std:', round(prediction_std_10[5928:6048].max(axis=0), 2))
print('Min std:', round(prediction_std_10[5928:6048].min(axis=0),2))
print('Mean std:', round(prediction_std_10[5928:6048].mean(axis=0), 2))

#quiet period
print('10E 70N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[2712:2832], prediction_mean_70[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_70[2712:2832], prediction_mean_70[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70[2712:2832], prediction_mean_40[2712:2832] ),2))

print('Max std:', round(prediction_std_70[2712:2832].max(axis=0), 2))
print('Min std:', round(prediction_std_70[2712:2832].min(axis=0),2))
print('Mean std:', round(prediction_std_70[2712:2832].mean(axis=0), 2))

print('10E 40N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[2712:2832], prediction_mean_40[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_40[2712:2832], prediction_mean_40[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_40[2712:2832], prediction_mean_40[2712:2832] ),2))

print('Max std:', round(prediction_std_40[2712:2832].max(axis=0), 2))
print('Min std:', round(prediction_std_40[2712:2832].min(axis=0),2))
print('Mean std:', round(prediction_std_40[2712:2832].mean(axis=0), 2))

print('10E 10N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[2712:2832], prediction_mean_10[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_10[2712:2832], prediction_mean_10[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10[2712:2832], prediction_mean_10[2712:2832] ),2))

print('Max std:', round(prediction_std_10[2712:2832].max(axis=0), 2))
print('Min std:', round(prediction_std_10[2712:2832].min(axis=0),2))
print('Mean std:', round(prediction_std_10[2712:2832].mean(axis=0), 2))

"""## Bayesian Neural Network with NLL loss

In the previous model, distributions were modeled over weights, but individual point estimates (VTEC) was provided for each input.
We can also model an output distribution, keeping the learning distribution over the weights as well. The last layer will be changed to parameterize the output. The negative log-likelihood (NLL) of this distribution is used as the loss function instead of the MSE. The NLL loss is also known as the negative logarithm of predictive density (NLPD). This approach is intended to model *aleatory uncertainty (data uncertainty)*, the irreducible noise in the data, and thus capture the stochastic nature of the process that generates the data.
"""

learning_rate = 0.01

def run_experiment(model, loss, num_epochs, num_batch_size, train_X, train_y):
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )
    count = 1
    xtrain_err = 0
    xval_err = 0
    splits=20 

    for train_index, test_index in tscv.split(train_X, train_y):
      X_train, X_val =train_X[train_index], train_X[test_index]
      y_train, y_val = train_y[train_index], train_y[test_index]
      
      history=model.fit(X_train, y_train, epochs=num_epochs, batch_size=num_batch, validation_data=(X_val, y_val))
      _, rmset = model.evaluate(X_train, y_train, verbose=0)
      print('Fold {}'.format(count))
      print(f"Train RMSE: {round(rmset, 3)}")
      
      _, rmsev = model.evaluate(X_val, y_val, verbose=0)
      print(f"Valid RMSE: {round(rmsev, 3)}")

      xtrain_err+=rmset
      xval_err+=rmsev
      
      count = count + 1
    
    train_rmse = xtrain_err/splits
    val_rmse = xval_err/splits
    print ('Average RMSE on train data', round(train_rmse,3))
    print ('Average RMSE on val data', round(val_rmse,3))

learning_rate = 0.1

def run_experiment(model, loss, num_epochs, num_batch_size, train_X, train_y):
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )
    count = 1
    xtrain_err = 0
    xval_err = 0
    splits=20 

    for train_index, test_index in tscv.split(train_X, train_y):
      X_train, X_val =train_X[train_index], train_X[test_index]
      y_train, y_val = train_y[train_index], train_y[test_index]
      
      history=model.fit(X_train, y_train, epochs=num_epochs, batch_size=num_batch, validation_data=(X_val, y_val))
      _, rmset = model.evaluate(X_train, y_train, verbose=0)
      print('Fold {}'.format(count))
      print(f"Train RMSE: {round(rmset, 3)}")
      
      _, rmsev = model.evaluate(X_val, y_val, verbose=0)
      print(f"Valid RMSE: {round(rmsev, 3)}")

      xtrain_err+=rmset
      xval_err+=rmsev
      
      count = count + 1
    
    train_rmse = xtrain_err/splits
    val_rmse = xval_err/splits
    print ('Average RMSE on train data', round(train_rmse,3))
    print ('Average RMSE on val data', round(val_rmse,3))

"""To provide the mean and variance as output of the model,  a custom output layer with two neurons will be created: for the mean VTEC output and for the standard deviation.  """

def create_probabilistic_bnn_model(input_dim, hidden_units, kl_weight):
    inputs = layers.Input(shape=(input_dim,))
    features = layers.BatchNormalization()(inputs)

    # Create hidden layers with weight uncertainty using the DenseVariational layer.
    for units in hidden_units:
        features = tfp.layers.DenseVariational(
            units=units,
            make_prior_fn=prior,
            make_posterior_fn=posterior,
            kl_weight=kl_weight,
            activation="sigmoid",
        )(features)

    distribution_params = layers.Dense(units=2)(features)
    outputs = tfp.layers.IndependentNormal(1)(distribution_params)

    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

def negative_loglikelihood(targets, estimated_distribution):
    return -estimated_distribution.log_prob(targets)

bnn_model_probabilistic_lowVTEC = create_probabilistic_bnn_model(16, [32], 1/train_X.size)
bnn_model_probabilistic_lowVTEC.summary()

bnn_model_probabilistic_midVTEC = create_probabilistic_bnn_model(16, [32], 1/train_X.size)
bnn_model_probabilistic_midVTEC.summary()

bnn_model_probabilistic_highVTEC = create_probabilistic_bnn_model(16, [32], 1/train_X.size)
bnn_model_probabilistic_highVTEC.summary()

num_epochs= 2000
num_batch= 500
#Learning_rate = 0.1

import time
start = time.time()

run_experiment(bnn_model_probabilistic_lowVTEC, negative_loglikelihood, num_epochs,num_batch,  train_X, train_y)

end = time.time()
print(end - start, "seconds")

num_epochs = 2000
num_batch= 500
#Learning_rate = 0.01

import time
start = time.time()

run_experiment(bnn_model_probabilistic_midVTEC, negative_loglikelihood, num_epochs,num_batch,  train_X, train_y)

end = time.time()
print(end - start, "seconds")

#Learning_rate=0.01
num_epochs =2000
num_batch= 500

import time
start = time.time()

run_experiment(bnn_model_probabilistic_highVTEC, negative_loglikelihood, num_epochs,num_batch,  train_X, train_y)

end = time.time()
print(end - start, "seconds")

"""### **Testing**"""

samples=10

prediction_distribution_10 = bnn_model_probabilistic_lowVTEC(test_X_10_scaled)
prediction_mean_aleatoric_10 = prediction_distribution_10.mean().numpy().tolist()
prediction_aleatoric_std_10 = prediction_distribution_10.stddev().numpy() 

# The 95% CI is computed as mean ± (2 * stdv)
upper_aleatoric_10 = (prediction_mean_aleatoric_10 + (2 * prediction_aleatoric_std_10)).tolist()
lower_aleatoric_10 = (prediction_mean_aleatoric_10 - (2 * prediction_aleatoric_std_10)).tolist()
prediction_aleatoric_stdv_10 = prediction_aleatoric_std_10.tolist()

for idx in range(samples):
    print(
        f"Prediction mean: {round(prediction_mean_aleatoric_10[idx][0], 2)}, "
        f"stddev: {round(prediction_aleatoric_stdv_10[idx][0], 2)}, "
        f"95% CI: [{round(upper_aleatoric_10[idx][0], 2)} - {round(lower_aleatoric_10[idx][0], 2)}]"
        f" - Actual: {y_10_test_new[idx]}"
    )

upper_d=np.array(upper_aleatoric_10)
lower_d=np.array(lower_aleatoric_10)
upperr_10 = upper_d.flatten()
lowerr_10 = lower_d.flatten()

prediction_distribution_40 = bnn_model_probabilistic_midVTEC(test_X_40_scaled)
prediction_mean_aleatoric_40 = prediction_distribution_40.mean().numpy().tolist()
prediction_aleatoric_std_40 = prediction_distribution_40.stddev().numpy() 

# The 95% CI is computed as mean ± (2 * stdv)
upper_aleatoric_40 = (prediction_mean_aleatoric_40 + (2 * prediction_aleatoric_std_40)).tolist()
lower_aleatoric_40 = (prediction_mean_aleatoric_40 - (2 * prediction_aleatoric_std_40)).tolist()
prediction_aleatoric_stdv_40 = prediction_aleatoric_std_40.tolist()

for idx in range(samples):
    print(
        f"Prediction mean: {round(prediction_mean_aleatoric_40[idx][0], 2)}, "
        f"stddev: {round(prediction_aleatoric_stdv_40[idx][0], 2)}, "
        f"95% CI: [{round(upper_aleatoric_40[idx][0], 2)} - {round(lower_aleatoric_40[idx][0], 2)}]"
        f" - Actual: {y_40_test_new[idx]}"
    )

upper_d=np.array(upper_aleatoric_40)
lower_d=np.array(lower_aleatoric_40)
upperr_40 = upper_d.flatten()
lowerr_40 = lower_d.flatten()

prediction_distribution_70 = bnn_model_probabilistic_highVTEC(test_X_70_scaled)
prediction_mean_aleatoric_70 = prediction_distribution_70.mean().numpy().tolist()
prediction_aleatoric_std_70 = prediction_distribution_70.stddev().numpy() 

# The 95% CI is computed as mean ± (2 * stdv)
upper_aleatoric_70 = (prediction_mean_aleatoric_70 + (2 * prediction_aleatoric_std_70)).tolist()
lower_aleatoric_70 = (prediction_mean_aleatoric_70 - (2 * prediction_aleatoric_std_70)).tolist()
prediction_aleatoric_stdv_70 = prediction_aleatoric_std_70.tolist()

for idx in range(samples):
    print(
        f"Prediction mean: {round(prediction_mean_aleatoric_70[idx][0], 2)}, "
        f"stddev: {round(prediction_aleatoric_stdv_70[idx][0], 2)}, "
        f"95% CI: [{round(upper_aleatoric_70[idx][0], 2)} - {round(lower_aleatoric_70[idx][0], 2)}]"
        f" - Actual: {y_70_test_new[idx]}"
    )

upper_d=np.array(upper_aleatoric_70)
lower_d=np.array(lower_aleatoric_70)
upperr_70 = upper_d.flatten()
lowerr_70 = lower_d.flatten()

"""### **In-out (percentage)**"""

#Test data of year 2017
diff_up_10 = upperr_10 - targets_10
diff_low_10 = lowerr_10 - targets_10
diff_up_40 = upperr_40 - targets_40
diff_low_40 = lowerr_40 - targets_40
diff_up_70 = upperr_70 - targets_70
diff_low_70 = lowerr_70 - targets_70

#Test data September 6-10,2017
diff_up_10_sep = upperr_10[5927:6047] - targets_10[5927:6047]
diff_low_10_sep = lowerr_10[5927:6047] - targets_10[5927:6047]
diff_up_40_sep = upperr_40[5927:6047] - targets_40[5927:6047]
diff_low_40_sep = lowerr_40[5927:6047] - targets_40[5927:6047]
diff_up_70_sep = upperr_70[5927:6047] - targets_70[5927:6047]
diff_low_70_sep = lowerr_70[5927:6047] - targets_70[5927:6047]

#Test data April 25-29,2017
diff_up_10_apr = upperr_10[2712:2832] - targets_10[2712:2832]
diff_low_10_apr = lowerr_10[2712:2832] - targets_10[2712:2832]
diff_up_40_apr = upperr_40[2712:2832] - targets_40[2712:2832]
diff_low_40_apr = lowerr_40[2712:2832]- targets_40[2712:2832]
diff_up_70_apr = upperr_70[2712:2832] - targets_70[2712:2832]
diff_low_70_apr = lowerr_70[2712:2832] - targets_70[2712:2832]

diff_up_10.size

data_2017_len=8758
data_apr_sep_len=120

a=0
b=0
for i in range(data_2017_len):
  if np.any((diff_up_10.values[i] >= 0) & (diff_low_10.values[i] <= 0)):
    a=a+1
  else:
    b=b+1

whole= a+b
percent_in_CI_10 = a/whole * 100
percent_out_CI_10 = b/whole * 100

#2017, 10E 10N
percent_in_CI_10

#September 6-10,2017, 10E 10N
percent_in_CI_10

#April 25-29,2017, 10E 10N
percent_in_CI_10

a=0
b=0
for i in range(data_2017_len):
  if np.any((diff_up_40.values[i] >= 0) & (diff_low_40.values[i] <= 0)):
    a=a+1
  else:
    b=b+1

whole= a+b
percent_in_CI_40 = a/whole * 100
percent_out_CI_40 = b/whole * 100

#2017, 10E 40N
percent_in_CI_40

#September 6-10,2017, 10E 40N
percent_in_CI_40

#April 25-29,2017, 10E 40N
percent_in_CI_40

a=0
b=0
for i in range(data_2017_len):
  if np.any((diff_up_70.values[i] >= 0) & (diff_low_70.values[i] <= 0)):
    a=a+1
  else:
    b=b+1

whole= a+b
percent_in_CI_70 = a/whole * 100
percent_out_CI_70 = b/whole * 100

#2017, 10E 70N
percent_in_CI_70

#September 6-10,2017, 10E 70N
percent_in_CI_70

#April 25-29,2017, 10E 70N
percent_in_CI_70

"""### **Figures**"""

from matplotlib.dates import DateFormatter
base = datetime.datetime(2017, 9, 6)
dates = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

plt.plot(dates,prediction_mean_aleatoric_10[5928:6048], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates,targets_10[5928:6048], 'o:',color='tab:orange', linewidth=1,markersize=4,  label='Ground truth')
plt.fill_between(dates, lowerr_10[5928:6048], upperr_10[5928:6048], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 62.0, 20.0)))
plt.rcParams ['figure.figsize'] = [10.0, 4.0]
plt.show()
plt.rcParams.update({'font.size': 16})

plt.plot(dates,prediction_mean_aleatoric_40[5928:6048], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates,targets_40[5928:6048], 'o:',color='tab:orange', linewidth=1,markersize=4,  label='Ground truth')
plt.fill_between(dates, lowerr_40[5928:6048], upperr_40[5928:6048], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 32.0, 15.0)))
plt.rcParams ['figure.figsize'] = [10.0, 4.0]
plt.show()
plt.rcParams.update({'font.size': 16})

plt.plot(dates,prediction_mean_aleatoric_70[5928:6048], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates,targets_70[5928:6048], 'o:',color='tab:orange', linewidth=1,markersize=4,  label='Ground truth')
plt.fill_between(dates, lowerr_70[5928:6048], upperr_70[5928:6048], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 22.0, 10.0)))
plt.rcParams ['figure.figsize'] = [10.0, 4.0]
plt.show()
plt.rcParams.update({'font.size': 16})

base = datetime.datetime(2017, 4, 25)
dates2 = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

plt.plot(dates2,prediction_mean_aleatoric_10[2712:2832], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates2, targets_10[2712:2832],  'o:',color='tab:orange', linewidth=1,markersize=4, label='Ground truth')
plt.fill_between(dates2, lowerr_10[2712:2832], upperr_10[2712:2832], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 62.0, 20.0)))
plt.show()
plt.rcParams.update({'font.size': 16})

plt.plot(dates2,prediction_mean_aleatoric_40[2712:2832], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates2, targets_40[2712:2832],  'o:',color='tab:orange', linewidth=1,markersize=4, label='Ground truth')
plt.fill_between(dates2, lowerr_40[2712:2832], upperr_40[2712:2832], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 32.0, 15.0)))
plt.show()
plt.rcParams.update({'font.size': 16})

plt.plot(dates2,prediction_mean_aleatoric_70[2712:2832], 'bo:',linewidth=1,markersize=4, label='VTEC mean (BNN NLDP)')
plt.plot(dates2, targets_70[2712:2832],  'o:',color='tab:orange', linewidth=1,markersize=4, label='Ground truth')
plt.fill_between(dates2, lowerr_70[2712:2832], upperr_70[2712:2832], alpha=0.3, color='tab:green',label='Uncertainty (BNN)')
plt.legend(loc="best", bbox_to_anchor=(1, 1, 0.5, 0))
plt.yticks((np.arange(0.0, 22.0, 10.0)))
plt.show()
plt.rcParams.update({'font.size': 16})

fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates, data_BNN2_70_2017['prediction_mean_aleatoric_70'][5927:6047], 'bo',linewidth=1,markersize=2, label='BNN NLPD')
axs[0].fill_between(dates,data_BNN2_70_2017['prediction_mean_aleatoric_70'] [5927:6047]- 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][5927:6047], data_BNN2_70_2017['prediction_mean_aleatoric_70'] [5927:6047] + 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][5927:6047],  alpha=0.4, color='tab:green', label='Uncertainty')
axs[0].plot(dates, data_BNN2_70_2017['targets_70'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 22.0, 10.0)))

axs[1].plot(dates, data_BNN2_40_2017['prediction_aleatoric_mean_40'][5927:6047], 'bo',linewidth=1,markersize=2, label='SE')
axs[1].fill_between(dates,data_BNN2_40_2017['prediction_aleatoric_mean_40'] [5927:6047]- 2*data_BNN2_40_2017['prediction_aleatoric_std_40'][5927:6047], data_BNN2_40_2017['prediction_aleatoric_mean_40'] [5927:6047] + 2*data_BNN2_40_2017['prediction_aleatoric_std_40'][5927:6047],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[1].plot(dates, data_BNN2_40_2017['targets_40'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 32.0, 10.0)))

axs[2].plot(dates, data_BNN2_10_2017['prediction_mean_aleatoric_10'][5927:6047], 'bo',linewidth=1,markersize=2, label='SE')
axs[2].fill_between(dates,data_BNN2_10_2017['prediction_mean_aleatoric_10'] [5927:6047]- 2*data_BNN2_10_2017['prediction_aleatoric_std_10'][5927:6047], data_BNN2_10_2017['prediction_mean_aleatoric_10'] [5927:6047] + 2*data_BNN2_10_2017['prediction_aleatoric_std_10'][5927:6047],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[2].plot(dates, data_BNN2_10_2017['targets_10'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 52.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()
plt.rcParams.update({'font.size': 15})

fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates2, data_BNN2_70_2017['prediction_mean_aleatoric_70'][2712:2832], 'bo',linewidth=1,markersize=2, label='BNN NLPD')
axs[0].fill_between(dates2,data_BNN2_70_2017['prediction_mean_aleatoric_70'] [2712:2832]- 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][2712:2832], data_BNN2_70_2017['prediction_mean_aleatoric_70'][2712:2832] + 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][2712:2832],  alpha=0.4, color='tab:green', label='Uncertainty')
axs[0].plot(dates2, data_BNN2_70_2017['targets_70'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 22.0, 10.0)))

axs[1].plot(dates2, data_BNN2_40_2017['prediction_aleatoric_mean_40'][2712:2832], 'bo',linewidth=1,markersize=2, label='BNN NLPD')
axs[1].fill_between(dates2,data_BNN2_40_2017['prediction_aleatoric_mean_40'] [2712:2832]- 2*data_BNN2_40_2017['prediction_aleatoric_std_40'][2712:2832], data_BNN2_40_2017['prediction_aleatoric_mean_40'][2712:2832] + 2*data_BNN2_40_2017['prediction_aleatoric_std_40'][2712:2832],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[1].plot(dates2, data_BNN2_40_2017['targets_40'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 32.0, 10.0)))

axs[2].plot(dates2, data_BNN2_10_2017['prediction_mean_aleatoric_10'][2712:2832], 'bo',linewidth=1,markersize=2, label='BNN NLPD')
axs[2].fill_between(dates2,data_BNN2_10_2017['prediction_mean_aleatoric_10'][2712:2832]- 2*data_BNN2_10_2017['prediction_aleatoric_std_10'][2712:2832], data_BNN2_10_2017['prediction_mean_aleatoric_10'] [2712:2832] + 2*data_BNN2_10_2017['prediction_aleatoric_std_10'][2712:2832],  alpha=0.4, color='tab:green', label='Ensemble spread')
axs[2].plot(dates2, data_BNN2_10_2017['targets_10'][2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='Ground truth (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 52.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0,6.0]
plt.show()

"""The plots below show a comparison of the mean VTEC forecast by the Super Ensemble (SE) model, the BNN model, and the BNN-NLL model. For further analysis and plots, see the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M."""

#April 25-29, 2017.
fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates2, diff_70_apr_SE,linewidth=1,markersize=2, label='SE')
axs[0].plot(dates2, diff_70_apr_BNN1,linewidth=1,markersize=2, label='BNN MSE')
axs[0].plot(dates2, diff_70_apr,linewidth=1,markersize=2, label='BNN NLPD')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(-5, 6.0, 5.0)))

axs[1].plot(dates2, diff_40_apr_SE, linewidth=1,markersize=2, label='SE')
axs[1].plot(dates2, diff_40_apr_BNN1, linewidth=1,markersize=2, label='BNN MSE')
axs[1].plot(dates2, diff_40_apr,linewidth=1,markersize=2, label='BNN NLPD')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(-10, 11.0, 5.0)))

axs[2].plot(dates2, diff_10_apr_SE, linewidth=1,markersize=2, label='SE')
axs[2].plot(dates2, diff_10_apr_BNN1, linewidth=1,markersize=2, label='BNN MSE')
axs[2].plot(dates2, diff_10_apr,linewidth=1,markersize=2, label='BNN NLPD')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(-10, 11.0, 5.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()

#September 6-10, 2017. Note: incorrect dates in the figure.
fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates, diff_70_sept_SE,linewidth=1,markersize=2, label='SE')
axs[0].plot(dates, diff_70_sept_BNN1,linewidth=1,markersize=2, label='BNN MSE')
axs[0].plot(dates, diff_70_sept,linewidth=1,markersize=2, label='BNN NLPD')
#axs[0].fill_between(dates2,diff_70_apr- 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][2712:2832], diff_70_apr + 2*data_BNN2_70_2017['prediction_std_aleatoric_70'][2712:2832],  alpha=0.4, color='tab:green', label='Uncertainty')
#axs[0].plot(dates2, data_BNN2_70_2017['targets_70'][5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(-5, 6.0, 5.0)))

axs[1].plot(dates, diff_40_sept_SE, linewidth=1,markersize=2, label='SE')
axs[1].plot(dates, diff_40_sept_BNN1, linewidth=1,markersize=2, label='BNN MSE')
axs[1].plot(dates, diff_40_sept, linewidth=1,markersize=2, label='BNN NLPD')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(-10, 11.0, 5.0)))

axs[2].plot(dates, diff_10_sept_SE, linewidth=1,markersize=2, label='SE')
axs[2].plot(dates, diff_10_sept_BNN1, linewidth=1,markersize=2, label='BNN MSE')
axs[2].plot(dates, diff_10_sept, linewidth=1,markersize=2, label='BNN NLPD')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(-10, 11.0, 5.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.5, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()

"""### **Statistics**"""

start = time.time()

y_pred = bnn_model_probabilistic_lowVTEC.predict(test_X)

end = time.time()
print(end - start, "seconds")

start = time.time()

y_pred = bnn_model_probabilistic_lowVTEC.predict(test_X)

end = time.time()
print(end - start, "seconds")

print('10E 70N (2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70, prediction_mean_aleatoric_70 )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_70, prediction_mean_aleatoric_70 )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70, prediction_mean_aleatoric_70 ),2))

print('Max std:', round(prediction_aleatoric_std_70.max(), 2))
print('Min std:', round(prediction_aleatoric_std_70.min(),2))
print('Mean std:', round(prediction_aleatoric_std_70.mean(), 2))
#NEW

print('10E 40N (2017)')
print('BNN2 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40, prediction_mean_aleatoric_40 )),2))
print('BNN2 Corr.:',  scipy.stats.pearsonr(targets_40, prediction_mean_aleatoric_40 )[0])
print('BNN2 MAE:',  round(metrics.mean_absolute_error(targets_40, prediction_mean_aleatoric_40 ),2))

print('Max std:', round(prediction_aleatoric_std_40.max(), 2))
print('Min std:', round(prediction_aleatoric_std_40.min(),2))
print('Mean std:', round(prediction_aleatoric_std_40.mean(), 2))

print('10E 10N (2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10, prediction_mean_aleatoric_10 )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_10, prediction_mean_aleatoric_10 )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10, prediction_mean_aleatoric_10 ),2))

print('Max std:', round(prediction_aleatoric_std_10.max(), 2))
print('Min std:', round(prediction_aleatoric_std_10.min(),2))
print('Mean std:', round(prediction_aleatoric_std_10.mean(), 2))

print('10E 70N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[5928:6048], prediction_mean_aleatoric_70[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_70[5928:6048], prediction_mean_aleatoric_70[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70[5928:6048], prediction_mean_aleatoric_70[5928:6048]),2))

print('Max std:', round(prediction_aleatoric_std_70[5928:6048].max(), 2))
print('Min std:', round(prediction_aleatoric_std_70[5928:6048].min(),2))
print('Mean std:', round(prediction_aleatoric_std_70[5928:6048].mean(), 2))

print('10E 40N (6-10 Sept 2017)')
print('BNN2 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[5928:6048], prediction_mean_aleatoric_40[5928:6048] )),2))
print('BNN2 Corr.:',  scipy.stats.pearsonr(targets_40[5928:6048], prediction_mean_aleatoric_40[5928:6048] )[0])
print('BNN2 MAE:',  round(metrics.mean_absolute_error(targets_40[5928:6048], prediction_mean_aleatoric_40[5928:6048]),2))

print('Max std:', round(prediction_aleatoric_std_40[5928:6048].max(), 2))
print('Min std:', round(prediction_aleatoric_std_40[5928:6048].min(),2))
print('Mean std:', round(prediction_aleatoric_std_40[5928:6048].mean(), 2))

print('10E 10N (6-10 Sept 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[5928:6048], prediction_mean_aleatoric_10[5928:6048] )),2))
print('BNN1 Corr.:',  scipy.stats.pearsonr(targets_10[5928:6048], prediction_mean_aleatoric_10[5928:6048] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10[5928:6048], prediction_mean_aleatoric_10[5928:6048]),2))

print('Max std:', round(prediction_aleatoric_std_10[5928:6048].max(), 2))
print('Min std:', round(prediction_aleatoric_std_10[5928:6048].min(),2))
print('Mean std:', round(prediction_aleatoric_std_10[5928:6048].mean(), 2))

#quiet period
print('10E 70N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_70[2712:2832], prediction_mean_aleatoric_70[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_70[2712:2832], prediction_mean_aleatoric_70[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_70[2712:2832], prediction_mean_aleatoric_70[2712:2832] ),2))

print('Max std:', round(prediction_aleatoric_std_70[2712:2832].max(), 2))
print('Min std:', round(prediction_aleatoric_std_70[2712:2832].min(),2))
print('Mean std:', round(prediction_aleatoric_std_70[2712:2832].mean(), 2))

print('10E 40N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_40[2712:2832], prediction_mean_aleatoric_40[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_40[2712:2832], prediction_mean_aleatoric_40[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_40[2712:2832], prediction_mean_aleatoric_40[2712:2832] ),2))

print('Max std:', round(prediction_aleatoric_std_40[2712:2832].max(), 2))
print('Min std:', round(prediction_aleatoric_std_40[2712:2832].min(),2))
print('Mean std:', round(prediction_aleatoric_std_40[2712:2832].mean(), 2))

print('10E 10N (25-29 April 2017)')
print('BNN1 RMSE:', round(np.sqrt(metrics.mean_squared_error(targets_10[2712:2832], prediction_mean_aleatoric_10[2712:2832] )),2))
print('BNN1 Corr.:', scipy.stats.pearsonr(targets_10[2712:2832], prediction_mean_aleatoric_10[2712:2832] )[0])
print('BNN1 MAE:',  round(metrics.mean_absolute_error(targets_10[2712:2832], prediction_mean_aleatoric_10[2712:2832] ),2))

print('Max std:', round(prediction_aleatoric_std_10[2712:2832].max(), 2))
print('Min std:', round(prediction_aleatoric_std_10[2712:2832].min(),2))
print('Mean std:', round(prediction_aleatoric_std_10[2712:2832].mean(), 2))